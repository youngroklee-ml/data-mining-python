[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Mining with Python",
    "section": "",
    "text": "1 Preface\nThis is a book to convert example code on https://youngroklee-ml.github.io/data-mining-techniques/ into Python.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "ch02_regression.html",
    "href": "ch02_regression.html",
    "title": "2  Regression",
    "section": "",
    "text": "2.1 Examples 2.3 - 2.5, 2.7, 2.10 - 2.11",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "ch02_regression.html#examples-2.3---2.5-2.7-2.10---2.11",
    "href": "ch02_regression.html#examples-2.3---2.5-2.7-2.10---2.11",
    "title": "2  Regression",
    "section": "",
    "text": "2.1.1 Load data\n\ndat1 = pd.read_csv(\"data/ch2_reg1.csv\")\n\n# print data\ndat1\n\n\n\n\n\n\n\n\n\nID\nage\nheight\nweight\n\n\n\n\n0\n1\n21\n170\n60\n\n\n1\n2\n47\n167\n65\n\n\n2\n3\n36\n173\n67\n\n\n3\n4\n15\n165\n54\n\n\n4\n5\n54\n168\n73\n\n\n5\n6\n25\n177\n71\n\n\n6\n7\n32\n169\n68\n\n\n7\n8\n18\n172\n62\n\n\n8\n9\n43\n171\n66\n\n\n9\n10\n28\n175\n68\n\n\n\n\n\n\n\n\n\n\n2.1.2 Ex 2.3: Regression coefficient\nDefine a linear regression model with smf.ols() by passing formula string and training data as arguments.\n\nmodel = smf.ols(\"weight ~ age + height\", data = dat1)\n\nEstimate the model by calling fit() method.\n\nmodel_fit = model.fit()\n\nLet’s see a summary of model estimation results.\n\nmodel_fit.summary()\n\n/opt/hostedtoolcache/Python/3.11.9/x64/lib/python3.11/site-packages/scipy/stats/_axis_nan_policy.py:531: UserWarning: kurtosistest only valid for n&gt;=20 ... continuing anyway, n=10\n  res = hypotest_fun_out(*samples, **kwds)\n\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nweight\nR-squared:\n0.822\n\n\nModel:\nOLS\nAdj. R-squared:\n0.771\n\n\nMethod:\nLeast Squares\nF-statistic:\n16.13\n\n\nDate:\nMon, 15 Jul 2024\nProb (F-statistic):\n0.00239\n\n\nTime:\n00:16:02\nLog-Likelihood:\n-22.163\n\n\nNo. Observations:\n10\nAIC:\n50.33\n\n\nDf Residuals:\n7\nBIC:\n51.23\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-108.1672\n42.123\n-2.568\n0.037\n-207.772\n-8.563\n\n\nage\n0.3291\n0.069\n4.753\n0.002\n0.165\n0.493\n\n\nheight\n0.9553\n0.244\n3.914\n0.006\n0.378\n1.532\n\n\n\n\n\n\nOmnibus:\n0.731\nDurbin-Watson:\n1.196\n\n\nProb(Omnibus):\n0.694\nJarque-Bera (JB):\n0.620\n\n\nSkew:\n0.477\nProb(JB):\n0.733\n\n\nKurtosis:\n2.239\nCond. No.\n8.72e+03\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 8.72e+03. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\nCoefficient estimates can be accessed with params attribute.\n\nmodel_fit.params\n\nIntercept   -108.167199\nage            0.329121\nheight         0.955291\ndtype: float64\n\n\n\n\n2.1.3 Ex 2.4: Variance of error terms\nscale attribute represents the estimate of error variance.\n\nmodel_fit.scale\n\nnp.float64(7.038464456795462)\n\n\n\n\n2.1.4 Ex 2.5: Test a model\nUse anova_lm() from statsmodels.api.stats module and pass the fitted linear regression model to conduct ANOVA test.\n\nsm.stats.anova_lm(model_fit)\n\n\n\n\n\n\n\n\n\ndf\nsum_sq\nmean_sq\nF\nPR(&gt;F)\n\n\n\n\nage\n1.0\n119.299334\n119.299334\n16.949625\n0.004476\n\n\nheight\n1.0\n107.831415\n107.831415\n15.320304\n0.005793\n\n\nResidual\n7.0\n49.269251\n7.038464\nNaN\nNaN\n\n\n\n\n\n\n\n\n\n\n2.1.5 Ex 2.7 Test regression coefficients\nOne of tables from summary() output represents coefficient-level statistics including t-test statstics.\n\nmodel_fit.summary().tables[1]\n\n/opt/hostedtoolcache/Python/3.11.9/x64/lib/python3.11/site-packages/scipy/stats/_axis_nan_policy.py:531: UserWarning: kurtosistest only valid for n&gt;=20 ... continuing anyway, n=10\n  res = hypotest_fun_out(*samples, **kwds)\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-108.1672\n42.123\n-2.568\n0.037\n-207.772\n-8.563\n\n\nage\n0.3291\n0.069\n4.753\n0.002\n0.165\n0.493\n\n\nheight\n0.9553\n0.244\n3.914\n0.006\n0.378\n1.532\n\n\n\n\n\n\n\n\n2.1.6 Ex 2.10 - 2.11 Mean prediction, confidence interval and prediction interval\nVariance-covariance matrix of coefficients:\n\nmodel_fit.cov_params()\n\n\n\n\n\n\n\n\n\nIntercept\nage\nheight\n\n\n\n\nIntercept\n1774.328062\n-0.671107\n-10.264885\n\n\nage\n-0.671107\n0.004795\n0.003035\n\n\nheight\n-10.264885\n0.003035\n0.059567\n\n\n\n\n\n\n\n\nCreate new data to predict:\n\nnewdata = pd.DataFrame({'age': [40, ], 'height': [170, ]})\n\nPredict mean response by calling predict() method.\n\nmodel_fit.predict(newdata)\n\n0    67.397178\ndtype: float64\n\n\nLet’s also get 95% confidence interval of mean response and 95% prediction interval. To do so, first let’s produce a prediction object by calling get_prediction() method.\n\nprediction_results = model_fit.get_prediction(newdata)\n\nsummary_frame() method produces data frame that include mean prediction, confidence interval, and prediction interval. To compute 95% confidence/prediction intervals, pass alpha=0.95 as an argument.\n\nprediction_results.summary_frame(alpha=0.95)\n\n\n\n\n\n\n\n\n\nmean\nmean_se\nmean_ci_lower\nmean_ci_upper\nobs_ci_lower\nobs_ci_upper\n\n\n\n\n0\n67.397178\n1.006575\n67.331762\n67.462594\n67.21277\n67.581587",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "ch02_regression.html#example-2.14-2.16",
    "href": "ch02_regression.html#example-2.14-2.16",
    "title": "2  Regression",
    "section": "2.2 Example 2.14, 2.16",
    "text": "2.2 Example 2.14, 2.16\n\n2.2.1 Load data\n\ndat1 = pd.read_csv(\"data/ch2_coil.csv\")\ndat1\n\n\n\n\n\n\n\n\n\ntemp\nthick\ny\n\n\n\n\n0\n540\n2\n52.5\n\n\n1\n660\n2\n50.2\n\n\n2\n610\n2\n51.3\n\n\n3\n710\n2\n49.1\n\n\n4\n570\n6\n50.8\n\n\n5\n700\n6\n48.7\n\n\n6\n560\n6\n51.2\n\n\n7\n600\n6\n50.8\n\n\n8\n680\n6\n49.3\n\n\n9\n530\n6\n51.5\n\n\n\n\n\n\n\n\n\n\n2.2.2 Ex 2.14: Estimate a model with an indicator variable\nUsing C() inside a formula means that the variable will be considered as a categorical variable. In such case, dummy variables are automatically created during the process of model estimation. Inside C(), you can pass the second argument Treatment(reference=6) to specify 6 as a base category, so the dummy variable value is 0 when original variable value is 6.\n\nmodel_fit = smf.ols(\"y ~ temp + C(thick, Treatment(reference=6))\", data=dat1).fit()\n\nLet’s check the estimated regression coefficients.\n\nmodel_fit.summary().tables[1]\n\n/opt/hostedtoolcache/Python/3.11.9/x64/lib/python3.11/site-packages/scipy/stats/_axis_nan_policy.py:531: UserWarning: kurtosistest only valid for n&gt;=20 ... continuing anyway, n=10\n  res = hypotest_fun_out(*samples, **kwds)\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n61.1080\n0.703\n86.864\n0.000\n59.444\n62.771\n\n\nC(thick, Treatment(reference=6))[T.2]\n0.8042\n0.150\n5.376\n0.001\n0.450\n1.158\n\n\ntemp\n-0.0177\n0.001\n-15.380\n0.000\n-0.020\n-0.015\n\n\n\n\n\n\n\n\n2.2.3 Ex 2.16: Interaction term\nUse * to estimate not only main effects but only interactions.\n\nmodel_fit = smf.ols(\"y ~ temp*C(thick, Treatment(reference=6))\", data=dat1).fit()\nmodel_fit.summary().tables[1]\n\n/opt/hostedtoolcache/Python/3.11.9/x64/lib/python3.11/site-packages/scipy/stats/_axis_nan_policy.py:531: UserWarning: kurtosistest only valid for n&gt;=20 ... continuing anyway, n=10\n  res = hypotest_fun_out(*samples, **kwds)\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n60.1364\n0.750\n80.187\n0.000\n58.301\n61.971\n\n\nC(thick, Treatment(reference=6))[T.2]\n3.2785\n1.210\n2.708\n0.035\n0.317\n6.240\n\n\ntemp\n-0.0161\n0.001\n-13.074\n0.000\n-0.019\n-0.013\n\n\ntemp:C(thick, Treatment(reference=6))[T.2]\n-0.0040\n0.002\n-2.055\n0.086\n-0.009\n0.001",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "ch03_regularized_regression.html",
    "href": "ch03_regularized_regression.html",
    "title": "3  Regularized regression",
    "section": "",
    "text": "3.1 Examples 3.1 - 3.2",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regularized regression</span>"
    ]
  },
  {
    "objectID": "ch03_regularized_regression.html#examples-3.1---3.2",
    "href": "ch03_regularized_regression.html#examples-3.1---3.2",
    "title": "3  Regularized regression",
    "section": "",
    "text": "3.1.1 Load data\n\ndat1 = pd.read_csv(\"data/ch3_dat1.csv\")\ndat1\n\n\n\n\n\n\n\n\n\nx1\nx2\ny\n\n\n\n\n0\n-6\n-3\n-3\n\n\n1\n-4\n-1\n-2\n\n\n2\n-3\n-2\n-1\n\n\n3\n1\n-4\n0\n\n\n4\n2\n1\n1\n\n\n5\n3\n2\n2\n\n\n6\n7\n7\n3\n\n\n\n\n\n\n\n\n\n\n3.1.2 Standardize inputs\nLet us convert data frame into input data matrix x and output array y. Additional, let N be the number of training data.\n\nx = dat1[['x1', 'x2']].to_numpy()\ny = dat1['y'].to_numpy()\nN = len(y)\n\n\nstd_x = (x - x.mean(axis=0)) / x.std(axis=0, ddof=1)\n\n\n\n3.1.3 Ex 3.1: Lasso\n\n3.1.3.1 statsmodels\nFirst, let us estimate a model by using {statsmodels}. After defining an OLS model, use fit_regularized() method with L1_wt=1 to give penalty on the L1-norm of cofficients.\n\ndef lasso(X, y, alpha):\n    model_fit = (sm\n                 .OLS(y, X)\n                 .fit_regularized(alpha=alpha, L1_wt=1))\n\n    return model_fit\n\nLet us fit models by varying penalization size.\n\nlambdas = np.arange(4) / (2 * N)\nlasso_fit = [lasso(std_x, y, l) for l in lambdas]\n[fit.params for fit in lasso_fit]\n\n[array([2.02007827, 0.1339426 ]),\n array([1.97361834, 0.08748266]),\n array([1.9271584 , 0.04102273]),\n array([1.87638317, 0.        ])]\n\n\n\n\n3.1.3.2 scikit-learn\n{scikit-learn} provides ElasticNet() to fit regularized regression model. Pass l1_ratio=1 to fit a Lasso model.\n\nlambdas = np.arange(4) / (2 * N)\nlasso_model = [ElasticNet(alpha=l, l1_ratio=1) for l in lambdas]\nlasso_fit = [model.fit(std_x, y) for model in lasso_model]\n[fit.coef_ for fit in lasso_fit]\n\n/opt/hostedtoolcache/Python/3.11.9/x64/lib/python3.11/site-packages/sklearn/base.py:1473: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n  return fit_method(estimator, *args, **kwargs)\n/opt/hostedtoolcache/Python/3.11.9/x64/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:697: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n  model = cd_fast.enet_coordinate_descent(\n/opt/hostedtoolcache/Python/3.11.9/x64/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.156e-01, tolerance: 2.800e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n  model = cd_fast.enet_coordinate_descent(\n\n\n[array([2.02007827, 0.1339426 ]),\n array([1.97388922, 0.08726767]),\n array([1.92747855, 0.04076864]),\n array([1.87638317, 0.        ])]\n\n\n\n\n\n3.1.4 Ex 3.2: Ridge\nDo similarly as Lasso, but use different argument L1_wt=0 and l1_ratio=0.\n\n3.1.4.1 statsmodels\n\ndef ridge(X, y, alpha):\n    model_fit = (sm\n                 .OLS(y, X)\n                 .fit_regularized(alpha=alpha, L1_wt=0))\n\n    return model_fit\n\n\nlambdas = np.arange(4) / N\nridge_fit = [ridge(std_x, y, l) for l in lambdas]\n[fit.params for fit in ridge_fit]\n\n[array([2.02007827, 0.1339426 ]),\n array([1.5071298 , 0.46375656]),\n array([1.2688007 , 0.54765121]),\n array([1.11772476, 0.56673637])]\n\n\n\n\n3.1.4.2 scikit-learn\n\nlambdas = np.arange(4) / N\nlasso_model = [ElasticNet(alpha=l, l1_ratio=0) for l in lambdas]\nlasso_fit = [model.fit(std_x, y) for model in lasso_model]\n[fit.coef_ for fit in lasso_fit]\n\n/opt/hostedtoolcache/Python/3.11.9/x64/lib/python3.11/site-packages/sklearn/base.py:1473: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n  return fit_method(estimator, *args, **kwargs)\n/opt/hostedtoolcache/Python/3.11.9/x64/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:697: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n  model = cd_fast.enet_coordinate_descent(\n/opt/hostedtoolcache/Python/3.11.9/x64/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.156e-01, tolerance: 2.800e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n  model = cd_fast.enet_coordinate_descent(\n/opt/hostedtoolcache/Python/3.11.9/x64/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.969e+00, tolerance: 2.800e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n  model = cd_fast.enet_coordinate_descent(\n/opt/hostedtoolcache/Python/3.11.9/x64/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.052e+00, tolerance: 2.800e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n  model = cd_fast.enet_coordinate_descent(\n/opt/hostedtoolcache/Python/3.11.9/x64/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.916e+00, tolerance: 2.800e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n  model = cd_fast.enet_coordinate_descent(\n\n\n[array([2.02007827, 0.1339426 ]),\n array([1.5071298 , 0.46375656]),\n array([1.2688007 , 0.54765121]),\n array([1.11772476, 0.56673637])]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regularized regression</span>"
    ]
  },
  {
    "objectID": "ch04_dimension_reduction.html",
    "href": "ch04_dimension_reduction.html",
    "title": "4  Dimension reduction",
    "section": "",
    "text": "4.1 Example 4.10",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension reduction</span>"
    ]
  },
  {
    "objectID": "ch04_dimension_reduction.html#example-4.10",
    "href": "ch04_dimension_reduction.html#example-4.10",
    "title": "4  Dimension reduction",
    "section": "",
    "text": "4.1.1 Load data\n\ndat2 = pd.read_csv(\"data/ch4_dat2.csv\", encoding=\"euc-kr\")\ndat2\n\n\n\n\n\n\n\n\n\nID\nX1\nX2\nX3\nX4\nX5\n\n\n\n\n0\nSK증권\n2.43\n11.10\n18.46\n441.67\n0.90\n\n\n1\n교보증권\n3.09\n9.95\n29.46\n239.43\n0.90\n\n\n2\n대신증권\n2.22\n6.86\n28.62\n249.36\n0.69\n\n\n3\n대우증권\n5.76\n23.19\n23.47\n326.09\n1.43\n\n\n4\n동부증권\n1.60\n5.64\n25.64\n289.98\n1.42\n\n\n5\n메리츠증권\n3.53\n10.64\n32.25\n210.10\n1.17\n\n\n6\n미래에셋증권\n4.26\n15.56\n24.40\n309.78\n0.81\n\n\n7\n부국증권\n3.86\n5.50\n70.74\n41.36\n0.81\n\n\n8\n브릿지증권\n4.09\n6.44\n64.38\n55.32\n0.32\n\n\n9\n삼성증권\n2.73\n10.68\n24.41\n309.59\n0.64\n\n\n10\n서울증권\n2.03\n4.50\n42.53\n135.12\n0.59\n\n\n11\n신영증권\n1.96\n8.92\n18.48\n441.19\n1.07\n\n\n12\n신흥증권\n3.25\n7.96\n40.42\n147.41\n1.19\n\n\n13\n우리투자증권\n2.01\n10.28\n17.46\n472.78\n1.25\n\n\n14\n유화증권\n2.28\n3.65\n63.71\n56.96\n0.12\n\n\n15\n한양증권\n4.51\n7.50\n63.52\n57.44\n0.80\n\n\n16\n한화증권\n3.29\n12.37\n24.47\n308.63\n0.57\n\n\n17\n현대증권\n1.73\n7.57\n19.59\n410.45\n1.19\n\n\n\n\n\n\n\n\n\n\n4.1.2 Principal component analysis with statsmodels\nTrain a PCA model.\n\npca_model = sm.PCA(dat2.drop(\"ID\", axis=1))\n\nPrint a loading matrix.\n\npca_model.loadings\n\n\n\n\n\n\n\n\n\ncomp_0\ncomp_1\ncomp_2\ncomp_3\ncomp_4\n\n\n\n\nX1\n0.076084\n0.779670\n0.000892\n0.140755\n0.605403\n\n\nX2\n-0.394630\n0.565412\n-0.295322\n-0.117644\n-0.650785\n\n\nX3\n0.569702\n0.162282\n0.241222\n0.637722\n-0.429217\n\n\nX4\n-0.559828\n-0.196543\n-0.256597\n0.748094\n0.149922\n\n\nX5\n-0.447785\n0.086368\n0.888118\n0.003668\n-0.057115\n\n\n\n\n\n\n\n\nDraw a scree plot.\n\nfig = pca_model.plot_scree(log_scale=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe x-axis represents number of principal component - 1. 0 in the x-axis represents the 1st principal component.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe y-axis values are different from the book example, because statsmodels returns eigenvalues multiplied by the number of observations.\n\nprint(\"Eigenvalue from statsmodels:\")\nprint(pca_model.eigenvals)\n\nprint(\"Eigenvalue in the book example:\")\nprint(pca_model.eigenvals / len(dat2))\n\nEigenvalue from statsmodels:\n0    49.706320\n1    28.901757\n2     9.910135\n3     1.153137\n4     0.328651\nName: eigenvals, dtype: float64\nEigenvalue in the book example:\n0    2.761462\n1    1.605653\n2    0.550563\n3    0.064063\n4    0.018258\nName: eigenvals, dtype: float64\n\n\n\n\nContribution\n\npca_model.rsquare\n\nncomp\n0    0.000000\n1    0.552292\n2    0.873423\n3    0.983536\n4    0.996348\n5    1.000000\nName: rsquare, dtype: float64",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension reduction</span>"
    ]
  },
  {
    "objectID": "ch04_dimension_reduction.html#example-4.12",
    "href": "ch04_dimension_reduction.html#example-4.12",
    "title": "4  Dimension reduction",
    "section": "4.2 Example 4.12",
    "text": "4.2 Example 4.12\n\n4.2.1 Load data\n\ndat3 = pd.read_csv(\"data/ch4_dat3.csv\")\ndat3\n\n\n\n\n\n\n\n\n\nx1\nx2\nx3\ny\n\n\n\n\n0\n-3\n-3\n5\n-30\n\n\n1\n-2\n-3\n7\n-20\n\n\n2\n0\n0\n4\n0\n\n\n3\n1\n2\n0\n5\n\n\n4\n2\n2\n-5\n10\n\n\n5\n2\n2\n-11\n35\n\n\n\n\n\n\n\n\n\nX = dat3.drop('y', axis=1)\ny = dat3['y']\n\n\n\n4.2.2 Principal component analysis with scikit-learn\nUse PCA() from skearn.decomposition to define a principal component analysis model. Pass n_components=2 argument to find up to two components.\nExtract principal component scores for training data by calling fit_transform() method. The results will be used as input to a linear regression model.\n\npca_model = PCA(n_components=2)\npc = pca_model.fit_transform(X)\npc\n\narray([[  6.23469918,  -1.98801689],\n       [  7.8320036 ,  -0.68170263],\n       [  3.69967748,   1.51516424],\n       [ -0.82086723,   2.03924933],\n       [ -5.69799841,   0.69402616],\n       [-11.24751463,  -1.57872021]])\n\n\n\n\n4.2.3 Linear regression with first two principal components\nUse LinearRegression() from sklearn.linear_model to define a linear regression model. Use principal component scores as input matrix.\n\npcr_model = LinearRegression()\npcr_model.fit(pc, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\nSee estimated intercept.\n\npcr_model.intercept_\n\nnp.float64(-5.822294013977115e-16)\n\n\nAlso, see estimated coefficient on each of two principal component scores.\n\npcr_model.coef_\n\narray([-2.9187982 ,  2.53920563])",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension reduction</span>"
    ]
  },
  {
    "objectID": "ch04_dimension_reduction.html#examples-4.14---4.15",
    "href": "ch04_dimension_reduction.html#examples-4.14---4.15",
    "title": "4  Dimension reduction",
    "section": "4.3 Examples 4.14 - 4.15",
    "text": "4.3 Examples 4.14 - 4.15\n\n4.3.1 Load data\n\ndat3 = pd.read_csv(\"data/ch4_dat3.csv\")\ndat3\n\n\n\n\n\n\n\n\n\nx1\nx2\nx3\ny\n\n\n\n\n0\n-3\n-3\n5\n-30\n\n\n1\n-2\n-3\n7\n-20\n\n\n2\n0\n0\n4\n0\n\n\n3\n1\n2\n0\n5\n\n\n4\n2\n2\n-5\n10\n\n\n5\n2\n2\n-11\n35\n\n\n\n\n\n\n\n\n\nX = dat3.drop('y', axis=1)\ny = dat3['y']\n\n\n\n4.3.2 Partial least squares regression\nUse PLSRegression() from sklearn.cross_decomposition module to define a partial least squares (PLS) regression model.\nIn this example, pass n_components=2 argument to use two latent variables. In addition, set scale=False to override default argument scale=True. Typically use the default argument that standardize inputs and outputs, but we will not do the standardization in this example.\n\npls_model = PLSRegression(n_components=2, scale=False)\npls_model.fit(X, y)\n\nPLSRegression(scale=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  PLSRegression?Documentation for PLSRegressioniFittedPLSRegression(scale=False) \n\n\nSee latent variable matrix.\n\npls_model.x_scores_\n\narray([[  6.3186736 ,  -2.01669993],\n       [  7.85142115,  -0.59033902],\n       [  3.62854523,   1.52677802],\n       [ -0.90713631,   1.95380974],\n       [ -5.72434291,   0.70830911],\n       [-11.16716076,  -1.58185792]])\n\n\nX-loading matrix.\n\npls_model.x_loadings_\n\narray([[-0.25399471,  0.54816492],\n       [-0.28607343,  0.73567618],\n       [ 0.92489821,  0.42380159]])\n\n\nWeight matrix for X.\n\npls_model.x_weights_\n\narray([[-0.28152506,  0.65106763],\n       [-0.31280562,  0.63219196],\n       [ 0.90713631,  0.4200527 ]])\n\n\ny-loading\n\npls_model.y_loadings_\n\narray([[-2.9271831 ,  2.49078301]])\n\n\nIntecept and coefficients on original input variables.\n\nprint(\"Intercept\")\nprint(pls_model.intercept_)\n\nprint(\"Coefficients\")\nprint(pls_model.coef_)\n\nIntercept\n[0.]\nCoefficients\n[[ 2.47539454  2.52323782 -1.70463588]]\n\n\n\n\n4.3.3 Prediction\nMake a prediction by calling predict() method.\n\ndat3.assign(\n    pred_y = pls_model.predict(X)\n)\n\n\n\n\n\n\n\n\n\nx1\nx2\nx3\ny\npred_y\n\n\n\n\n0\n-3\n-3\n5\n-30\n-23.519076\n\n\n1\n-2\n-3\n7\n-20\n-24.452954\n\n\n2\n0\n0\n4\n0\n-6.818544\n\n\n3\n1\n2\n0\n5\n7.521870\n\n\n4\n2\n2\n-5\n10\n18.520444\n\n\n5\n2\n2\n-11\n35\n28.748259",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension reduction</span>"
    ]
  },
  {
    "objectID": "ch05_classification.html",
    "href": "ch05_classification.html",
    "title": "5  Classification",
    "section": "",
    "text": "5.1 Example 5.2",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "ch05_classification.html#example-5.2",
    "href": "ch05_classification.html#example-5.2",
    "title": "5  Classification",
    "section": "",
    "text": "5.1.1 Load data\n\ndat1 = pd.read_csv(\"data/ch7_dat1.csv\")\ndat1\n\n\n\n\n\n\n\n\n\nID\nX1\nX2\nclass\n\n\n\n\n0\n1\n5\n7\n1\n\n\n1\n2\n4\n3\n2\n\n\n2\n3\n7\n8\n2\n\n\n3\n4\n8\n6\n2\n\n\n4\n5\n3\n6\n1\n\n\n5\n6\n2\n5\n1\n\n\n6\n7\n6\n6\n1\n\n\n7\n8\n9\n6\n2\n\n\n8\n9\n5\n4\n2\n\n\n\n\n\n\n\n\n\nX = dat1[['X1', 'X2']]\ny = LabelBinarizer().fit_transform(dat1['class'])\n\n\n\n5.1.2 Training and testing data\nLet us use the first 7 observations as training data, while remaining 2 observations as testing data.\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, \n    test_size=2, train_size=7, \n    random_state=None, shuffle=False\n)\n\n\n\n5.1.3 k-nearest neighbors classifier\nUse KNeighborsClassifier() from sklearn.neighbors module to define a kNN model for a classification problem.\nSet n_neighbors=3 to train 3-NN in this example.\n\nknn_model = KNeighborsClassifier(n_neighbors=3)\n\nTrain the 3-NN classifier on training data.\n\nknn_model.fit(X_train, y_train)\n\n/opt/hostedtoolcache/Python/3.11.9/x64/lib/python3.11/site-packages/sklearn/neighbors/_classification.py:238: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  return self._fit(X, y)\n\n\nKNeighborsClassifier(n_neighbors=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KNeighborsClassifier?Documentation for KNeighborsClassifieriFittedKNeighborsClassifier(n_neighbors=3) \n\n\nUse predict() method to make a prediction on training data.\n\npd.DataFrame(X_train).assign(\n    observed_class = y_train,\n    pred_class = knn_model.predict(X_train)\n)\n\n\n\n\n\n\n\n\n\nX1\nX2\nobserved_class\npred_class\n\n\n\n\n0\n5\n7\n0\n0\n\n\n1\n4\n3\n1\n0\n\n\n2\n7\n8\n1\n1\n\n\n3\n8\n6\n1\n1\n\n\n4\n3\n6\n0\n0\n\n\n5\n2\n5\n0\n0\n\n\n6\n6\n6\n0\n0\n\n\n\n\n\n\n\n\nAlso, maka a prediction on testing data.\n\npd.DataFrame(X_test).assign(\n    observed_class = y_test,\n    pred_class = knn_model.predict(X_test)\n)\n\n\n\n\n\n\n\n\n\nX1\nX2\nobserved_class\npred_class\n\n\n\n\n7\n9\n6\n1\n1\n\n\n8\n5\n4\n1\n0",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "ch05_classification.html#examples-5.3---5.4",
    "href": "ch05_classification.html#examples-5.3---5.4",
    "title": "5  Classification",
    "section": "5.2 Examples 5.3 - 5.4",
    "text": "5.2 Examples 5.3 - 5.4\n\n5.2.1 Load data\n\ndat3 = pd.read_csv(\"data/ch5_dat3.csv\")\ndat3\n\n\n\n\n\n\n\n\n\nID\ngender\nage_gr\nclass\n\n\n\n\n0\n1\nM\n2\n1\n\n\n1\n2\nM\n2\n2\n\n\n2\n3\nM\n3\n1\n\n\n3\n4\nM\n4\n1\n\n\n4\n5\nF\n1\n1\n\n\n5\n6\nF\n2\n2\n\n\n6\n7\nF\n2\n1\n\n\n7\n8\nF\n3\n2\n\n\n8\n9\nF\n4\n2\n\n\n\n\n\n\n\n\n\n\n5.2.2 Preprocess data\nEncode categorical values to integer variable by using factorize() method from pandas module.\n\nX = dat3[['gender', 'age_gr']].copy()\nX.gender, _ = pd.factorize(X.gender)\nX.age_gr, _ = pd.factorize(X.age_gr)\n\n\n\n5.2.3 Ex 5.3: Naive Bayes classifier\n\n5.2.3.1 Estimation\nUse CategoricalNB() to define a naive Bayes classifier. Pass alpha=0 argument to do not apply smoothing.\n\nnb_model = CategoricalNB(alpha=0)\nnb_model.fit(X, dat3['class'])\n\n/opt/hostedtoolcache/Python/3.11.9/x64/lib/python3.11/site-packages/sklearn/naive_bayes.py:1504: RuntimeWarning: divide by zero encountered in log\n  np.log(smoothed_cat_count) - np.log(smoothed_class_count.reshape(-1, 1))\n\n\nCategoricalNB(alpha=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  CategoricalNB?Documentation for CategoricalNBiFittedCategoricalNB(alpha=0) \n\n\n\n\n5.2.3.2 Posterior\nCall predict_proba() method to predict posterior probability of belonging to each class.\n\nnb_model.predict_proba(X)\n\narray([[0.70588235, 0.29411765],\n       [0.70588235, 0.29411765],\n       [0.70588235, 0.29411765],\n       [0.70588235, 0.29411765],\n       [1.        , 0.        ],\n       [0.34782609, 0.65217391],\n       [0.34782609, 0.65217391],\n       [0.34782609, 0.65217391],\n       [0.34782609, 0.65217391]])\n\n\n\n\n5.2.3.3 Classification\nCall predict() method to make classification.\n\nnb_model.predict(X)\n\narray([1, 1, 1, 1, 1, 2, 2, 2, 2])\n\n\n\n\n\n5.2.4 Ex 5.4: Evaluation metrics\n\ny = dat3['class']\ny_pred = nb_model.predict(X)\n\n\n5.2.4.1 Confusion matrix\n\nconfusion_matrix(y, y_pred)\n\narray([[4, 1],\n       [1, 3]])\n\n\n\n\n5.2.4.2 Accuracy\n\naccuracy_score(y, y_pred)\n\n0.7777777777777778\n\n\n\n\n5.2.4.3 Sensitivity (Recall)\nSensitivity is the same to precision. Use precision_score() in sklearn.metrics module. Set class 1 to be a positive event in this example by passing pos_label=1 argument.\n\nprecision_score(y, y_pred, pos_label=1)\n\nnp.float64(0.8)\n\n\n\n\n5.2.4.4 Specificity\nSpecificity is the same to sensitivity with different event level, by passing pos_label=2 argument.\n\nprecision_score(y, y_pred, pos_label=2)\n\nnp.float64(0.75)\n\n\n\n\n5.2.4.5 F1-score\nF1 score is a harmonic mean of precision and recall.\nYou can either indirectly compute by using precision and recall\n\nprecision = precision_score(y, y_pred)\nrecall = recall_score(y, y_pred)\n2 * precision * recall / (precision + recall)\n\nnp.float64(0.8000000000000002)\n\n\nor directly compute by using f1_score function.\n\nf1_score(y, y_pred)\n\nnp.float64(0.8)\n\n\n\n\n5.2.4.6 ROC curve and AUC\nComputing a ROC curve requires posterior probability.\n\nprob = nb_model.predict_proba(X)[:, 0]\n\nVisualize ROC curve.\n\nfpr, tpr, thresholds = roc_curve(y, y_score = prob, pos_label=1)\nroc_auc = auc(fpr, tpr)\ndisplay = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, pos_label=1)\ndisplay.plot()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "ch06_logistic_regression.html",
    "href": "ch06_logistic_regression.html",
    "title": "6  Logistic regression",
    "section": "",
    "text": "6.1 Examples 6.1, 6.3",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "ch06_logistic_regression.html#examples-6.1-6.3",
    "href": "ch06_logistic_regression.html#examples-6.1-6.3",
    "title": "6  Logistic regression",
    "section": "",
    "text": "6.1.1 Load data\n\ndat1 = pd.read_csv(\"data/ch6_dat1.csv\")\ndat1\n\n\n\n\n\n\n\n\n\nBreak\nSleep\nCircle\nClass\n\n\n\n\n0\n0\n8\n2\nExcellent\n\n\n1\n1\n7\n1\nExcellent\n\n\n2\n0\n9\n0\nExcellent\n\n\n3\n1\n6\n4\nExcellent\n\n\n4\n1\n8\n2\nExcellent\n\n\n5\n0\n7\n3\nExcellent\n\n\n6\n0\n7\n0\nAverage\n\n\n7\n1\n6\n1\nAverage\n\n\n8\n0\n7\n2\nAverage\n\n\n9\n0\n8\n1\nAverage\n\n\n10\n0\n5\n2\nAverage\n\n\n11\n1\n8\n0\nAverage\n\n\n12\n0\n6\n3\nAverage\n\n\n13\n1\n7\n2\nAverage\n\n\n14\n0\n6\n1\nAverage\n\n\n\n\n\n\n\n\n\nX = dat1[['Break', 'Sleep', 'Circle']]\ny = LabelBinarizer().fit_transform(dat1['Class']).ravel()\ndat1['Class'] = y\n\n\n\n6.1.2 Ex 6.1: Logit model\n\n6.1.2.1 Use scikit-learn\nModel training:\n\nlogit_model = LogisticRegression(penalty = None)\nlogit_fit = logit_model.fit(X, y)\n\nIntercept:\n\nlogit_fit.intercept_\n\narray([-30.51046112])\n\n\nCoefficients on variables:\n\nlogit_fit.coef_\n\narray([[2.03119671, 3.47062988, 2.41437184]])\n\n\nPrediction:\n\ndat1.assign(\n    pred_logit = np.diff(logit_fit.predict_log_proba(X), axis=1),\n    pred_prob = logit_fit.predict_proba(X)[:, 1],\n    pred_class = logit_fit.predict(X)\n)\n\n\n\n\n\n\n\n\n\nBreak\nSleep\nCircle\nClass\npred_logit\npred_prob\npred_class\n\n\n\n\n0\n0\n8\n2\n1\n2.083322\n0.889272\n1\n\n\n1\n1\n7\n1\n1\n-1.770483\n0.145482\n0\n\n\n2\n0\n9\n0\n1\n0.725208\n0.673753\n1\n\n\n3\n1\n6\n4\n1\n2.002002\n0.881007\n1\n\n\n4\n1\n8\n2\n1\n4.114518\n0.983929\n1\n\n\n5\n0\n7\n3\n1\n1.027064\n0.736346\n1\n\n\n6\n0\n7\n0\n0\n-6.216052\n0.001993\n0\n\n\n7\n1\n6\n1\n0\n-5.241113\n0.005266\n0\n\n\n8\n0\n7\n2\n0\n-1.387308\n0.199838\n0\n\n\n9\n0\n8\n1\n0\n-0.331050\n0.417985\n0\n\n\n10\n0\n5\n2\n0\n-8.328568\n0.000241\n0\n\n\n11\n1\n8\n0\n0\n-0.714225\n0.328666\n0\n\n\n12\n0\n6\n3\n0\n-2.443566\n0.079910\n0\n\n\n13\n1\n7\n2\n0\n0.643888\n0.655632\n1\n\n\n14\n0\n6\n1\n0\n-7.272310\n0.000694\n0\n\n\n\n\n\n\n\n\n\n\n6.1.2.2 Use statsmodels\nModel training:\n\nlogit_fit = smf.logit(\"Class ~ Break + Sleep + Circle\", data = dat1).fit()\n\nOptimization terminated successfully.\n         Current function value: 0.347275\n         Iterations 8\n\n\nCoefficient table:\n\nlogit_fit.summary().tables[1]\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-30.5108\n18.021\n-1.693\n0.090\n-65.832\n4.811\n\n\nBreak\n2.0313\n1.984\n1.024\n0.306\n-1.857\n5.920\n\n\nSleep\n3.4707\n2.075\n1.672\n0.094\n-0.597\n7.538\n\n\nCircle\n2.4144\n1.397\n1.729\n0.084\n-0.323\n5.152\n\n\n\n\n\n\nPrediction:\n\ndat1.assign(\n    pred_logit = logit_fit.predict(dat1, which = 'linear'),\n    pred_prob = logit_fit.predict(dat1, which = 'mean'),\n)\n\n\n\n\n\n\n\n\n\nBreak\nSleep\nCircle\nClass\npred_logit\npred_prob\n\n\n\n\n0\n0\n8\n2\n1\n2.083307\n0.889270\n\n\n1\n1\n7\n1\n1\n-1.770473\n0.145484\n\n\n2\n0\n9\n0\n1\n0.725205\n0.673752\n\n\n3\n1\n6\n4\n1\n2.002017\n0.881009\n\n\n4\n1\n8\n2\n1\n4.114585\n0.983930\n\n\n5\n0\n7\n3\n1\n1.027023\n0.736338\n\n\n6\n0\n7\n0\n0\n-6.216138\n0.001993\n\n\n7\n1\n6\n1\n0\n-5.241144\n0.005266\n\n\n8\n0\n7\n2\n0\n-1.387364\n0.199829\n\n\n9\n0\n8\n1\n0\n-0.331080\n0.417978\n\n\n10\n0\n5\n2\n0\n-8.328707\n0.000241\n\n\n11\n1\n8\n0\n0\n-0.714188\n0.328674\n\n\n12\n0\n6\n3\n0\n-2.443648\n0.079904\n\n\n13\n1\n7\n2\n0\n0.643914\n0.655638\n\n\n14\n0\n6\n1\n0\n-7.272422\n0.000694\n\n\n\n\n\n\n\n\n\n\n\n6.1.3 Ex 6.3: Probit model\n\n6.1.3.1 Use statsmodels\nModel training:\n\nprobit_fit = smf.probit(\"Class ~ Break + Sleep + Circle\", data = dat1).fit()\n\nOptimization terminated successfully.\n         Current function value: 0.346018\n         Iterations 8\n\n\nCoefficient table:\n\nprobit_fit.summary().tables[1]\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-17.8505\n10.085\n-1.770\n0.077\n-37.617\n1.916\n\n\nBreak\n1.2607\n1.174\n1.074\n0.283\n-1.040\n3.561\n\n\nSleep\n2.0313\n1.165\n1.744\n0.081\n-0.252\n4.314\n\n\nCircle\n1.4144\n0.787\n1.797\n0.072\n-0.128\n2.957\n\n\n\n\n\n\nPrediction:\n\ndat1.assign(\n    pred_probit = probit_fit.predict(dat1, which = 'linear'),\n    pred_prob = probit_fit.predict(dat1, which = 'mean'),\n)\n\n\n\n\n\n\n\n\n\nBreak\nSleep\nCircle\nClass\npred_probit\npred_prob\n\n\n\n\n0\n0\n8\n2\n1\n1.228967\n8.904579e-01\n\n\n1\n1\n7\n1\n1\n-0.956056\n1.695219e-01\n\n\n2\n0\n9\n0\n1\n0.431420\n6.669183e-01\n\n\n3\n1\n6\n4\n1\n1.255928\n8.954289e-01\n\n\n4\n1\n8\n2\n1\n2.489707\n9.936076e-01\n\n\n5\n0\n7\n3\n1\n0.612077\n7.297567e-01\n\n\n6\n0\n7\n0\n0\n-3.631233\n1.410351e-04\n\n\n7\n1\n6\n1\n0\n-2.987383\n1.406886e-03\n\n\n8\n0\n7\n2\n0\n-0.802360\n2.111725e-01\n\n\n9\n0\n8\n1\n0\n-0.185470\n4.264302e-01\n\n\n10\n0\n5\n2\n0\n-4.865012\n5.722487e-07\n\n\n11\n1\n8\n0\n0\n-0.339167\n3.672420e-01\n\n\n12\n0\n6\n3\n0\n-1.419249\n7.791321e-02\n\n\n13\n1\n7\n2\n0\n0.458381\n6.766605e-01\n\n\n14\n0\n6\n1\n0\n-4.248123\n1.077847e-05",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "ch06_logistic_regression.html#example-6.4",
    "href": "ch06_logistic_regression.html#example-6.4",
    "title": "6  Logistic regression",
    "section": "6.2 Example 6.4",
    "text": "6.2 Example 6.4\n\n6.2.1 Load data\n\ndat2 = pd.read_csv(\"data/ch6_dat2.csv\")\ndat2\n\n\n\n\n\n\n\n\n\nX1\nX2\nY\n\n\n\n\n0\n9.33\n5.02\n1\n\n\n1\n9.91\n5.01\n1\n\n\n2\n11.88\n4.94\n1\n\n\n3\n11.54\n5.12\n1\n\n\n4\n11.66\n5.03\n1\n\n\n5\n12.43\n4.94\n2\n\n\n6\n10.32\n5.13\n2\n\n\n7\n10.15\n4.87\n1\n\n\n8\n9.83\n5.13\n2\n\n\n9\n10.79\n4.94\n3\n\n\n10\n10.57\n4.93\n3\n\n\n11\n8.66\n5.02\n3\n\n\n12\n9.59\n5.01\n3\n\n\n13\n9.35\n4.94\n3\n\n\n14\n10.20\n5.12\n2\n\n\n15\n12.20\n4.93\n2\n\n\n16\n9.80\n5.00\n1\n\n\n17\n8.80\n5.01\n3\n\n\n\n\n\n\n\n\n\nX = dat2[['X1', 'X2']]\ny = LabelEncoder().fit_transform(dat2['Y']).ravel()\ndat2['Y'] = y\n\n\n\n6.2.2 Use scikit-learn\nStill use LogisticRegression() to fit multinomial logistic regression.\n\nmnlogit_model = LogisticRegression(penalty=None)\nmnlogit_fit = mnlogit_model.fit(X, y)\n\nIntercept:\n\nmnlogit_fit.intercept_\n\narray([ -4.43704699, -78.4303051 ,  82.86735209])\n\n\nCoefficients on variables:\n\nmnlogit_fit.coef_\n\narray([[  0.21911162,   0.55762967],\n       [  1.02626459,  13.47864095],\n       [ -1.2453762 , -14.03627062]])\n\n\nPrediction:\n\npd.concat(\n    [dat2, \n     pd.DataFrame(\n        mnlogit_fit.predict_proba(X), \n        columns=['pred_prob_0', 'pred_prob_1', 'pred_prob_2'])],\n    axis=1\n).assign(\n    pred_class = mnlogit_fit.predict(X)\n)\n\n\n\n\n\n\n\n\n\nX1\nX2\nY\npred_prob_0\npred_prob_1\npred_prob_2\npred_class\n\n\n\n\n0\n9.33\n5.02\n0\n0.375457\n0.075876\n0.548667\n2\n\n\n1\n9.91\n5.01\n0\n0.498308\n0.141332\n0.360360\n0\n\n\n2\n11.88\n4.94\n0\n0.596950\n0.336080\n0.066970\n0\n\n\n3\n11.54\n5.12\n0\n0.185441\n0.812084\n0.002475\n1\n\n\n4\n11.66\n5.03\n0\n0.392202\n0.591469\n0.016329\n1\n\n\n5\n12.43\n4.94\n1\n0.518741\n0.455253\n0.026007\n0\n\n\n6\n10.32\n5.13\n1\n0.341264\n0.635240\n0.023496\n1\n\n\n7\n10.15\n4.87\n0\n0.200717\n0.011320\n0.787963\n2\n\n\n8\n9.83\n5.13\n1\n0.417626\n0.523444\n0.058931\n1\n\n\n9\n10.79\n4.94\n2\n0.559541\n0.130691\n0.309768\n0\n\n\n10\n10.57\n4.93\n2\n0.486386\n0.083591\n0.430023\n0\n\n\n11\n8.66\n5.02\n2\n0.199361\n0.023460\n0.777180\n2\n\n\n12\n9.59\n5.01\n2\n0.421133\n0.092255\n0.486613\n2\n\n\n13\n9.35\n4.94\n2\n0.177488\n0.012966\n0.809546\n2\n\n\n14\n10.20\n5.12\n1\n0.387631\n0.575554\n0.036815\n1\n\n\n15\n12.20\n4.93\n1\n0.580784\n0.372030\n0.047186\n0\n\n\n16\n9.80\n5.00\n0\n0.452257\n0.103146\n0.444597\n0\n\n\n17\n8.80\n5.01\n2\n0.208748\n0.024169\n0.767083\n2\n\n\n\n\n\n\n\n\n\n\n6.2.3 Use statsmodels\nUse mnlogit() to train a model:\n\nmnlogit_fit = smf.mnlogit(\"Y ~ X1 + X2\", data=dat2).fit()\n\nOptimization terminated successfully.\n         Current function value: 0.783682\n         Iterations 7\n\n\nCoefficient table:\n\nmnlogit_fit.summary().tables[1]\n\n\n\n\n\nY=1\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-73.9935\n52.464\n-1.410\n0.158\n-176.822\n28.835\n\n\nX1\n0.8071\n0.745\n1.083\n0.279\n-0.653\n2.267\n\n\nX2\n12.9212\n9.463\n1.365\n0.172\n-5.625\n31.468\n\n\nY=2\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n87.3081\n69.844\n1.250\n0.211\n-49.583\n224.199\n\n\nX1\n-1.4646\n0.881\n-1.663\n0.096\n-3.191\n0.261\n\n\nX2\n-14.5944\n13.161\n-1.109\n0.267\n-40.390\n11.201\n\n\n\n\n\n\nPrediction:\n\npd.concat([dat2, mnlogit_fit.predict(dat2)], axis=1)\n\n\n\n\n\n\n\n\n\nX1\nX2\nY\n0\n1\n2\n\n\n\n\n0\n9.33\n5.02\n0\n0.375445\n0.075881\n0.548674\n\n\n1\n9.91\n5.01\n0\n0.498305\n0.141341\n0.360354\n\n\n2\n11.88\n4.94\n0\n0.596970\n0.336069\n0.066961\n\n\n3\n11.54\n5.12\n0\n0.185443\n0.812083\n0.002474\n\n\n4\n11.66\n5.03\n0\n0.392211\n0.591463\n0.016326\n\n\n5\n12.43\n4.94\n1\n0.518766\n0.455231\n0.026002\n\n\n6\n10.32\n5.13\n1\n0.341250\n0.635257\n0.023492\n\n\n7\n10.15\n4.87\n0\n0.200711\n0.011320\n0.787970\n\n\n8\n9.83\n5.13\n1\n0.417607\n0.523469\n0.058924\n\n\n9\n10.79\n4.94\n2\n0.559551\n0.130694\n0.309755\n\n\n10\n10.57\n4.93\n2\n0.486393\n0.083593\n0.430014\n\n\n11\n8.66\n5.02\n2\n0.199343\n0.023461\n0.777197\n\n\n12\n9.59\n5.01\n2\n0.421124\n0.092261\n0.486615\n\n\n13\n9.35\n4.94\n2\n0.177475\n0.012966\n0.809558\n\n\n14\n10.20\n5.12\n1\n0.387616\n0.575574\n0.036810\n\n\n15\n12.20\n4.93\n1\n0.580808\n0.372013\n0.047179\n\n\n16\n9.80\n5.00\n0\n0.452252\n0.103153\n0.444595\n\n\n17\n8.80\n5.01\n2\n0.208731\n0.024171\n0.767099\n\n\n\n\n\n\n\n\nConfusion matrix:\n\nmnlogit_fit.pred_table()\n\narray([[3., 2., 2.],\n       [2., 3., 0.],\n       [2., 0., 4.]])",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "ch06_logistic_regression.html#example-6.5",
    "href": "ch06_logistic_regression.html#example-6.5",
    "title": "6  Logistic regression",
    "section": "6.3 Example 6.5",
    "text": "6.3 Example 6.5\n\n6.3.1 Load data\n\ndat3 = pd.read_csv(\"data/ch6_dat3.csv\")\nX = dat3[['N', 'L']]\ny = OrdinalEncoder().fit_transform(dat3[['Y']])\ndat3['Y'] = y\n\n\n\n6.3.2 Use statsmodels\nTrain a model. Use statsmodels.miscmodels.ordinal_model.OrderedModel(). Set distr='logit' to use logit link function.\n\nordinal_fit = OrderedModel(y, X, distr='logit').fit()\n\nOptimization terminated successfully.\n         Current function value: 0.536771\n         Iterations: 244\n         Function evaluations: 408\n\n\nCoefficient tables:\n\nordinal_fit.summary().tables[1]\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nN\n-0.2236\n0.146\n-1.529\n0.126\n-0.510\n0.063\n\n\nL\n-0.2999\n0.137\n-2.188\n0.029\n-0.569\n-0.031\n\n\n0.0/1.0\n-13.0352\n6.459\n-2.018\n0.044\n-25.695\n-0.375\n\n\n1.0/2.0\n0.4924\n0.649\n0.759\n0.448\n-0.780\n1.765\n\n\n\n\n\n\nPrediction:\n\nordinal_fit.predict(X)\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0.002612\n0.010659\n0.986729\n\n\n1\n0.011593\n0.045223\n0.943184\n\n\n2\n0.190484\n0.356722\n0.452793\n\n\n3\n0.825197\n0.135192\n0.039612\n\n\n4\n0.012374\n0.048084\n0.939542\n\n\n5\n0.053137\n0.170598\n0.776265\n\n\n6\n0.529599\n0.322957\n0.147444\n\n\n7\n0.957603\n0.033850\n0.008547\n\n\n8\n0.104948\n0.270912\n0.624140\n\n\n9\n0.344343\n0.385191\n0.270467\n\n\n10\n0.913317\n0.068539\n0.018144\n\n\n11\n0.995291\n0.003788\n0.000920\n\n\n\n\n\n\n\n\nConfusion matrix:\n\nordinal_fit.pred_table()\n\n\n\n\n\n\n\n\nrow_0\n0\n1\n2\nAll\n\n\ncol_0\n\n\n\n\n\n\n\n\n0\n4\n0\n1\n5\n\n\n1\n1\n0\n1\n2\n\n\n2\n0\n1\n4\n5\n\n\nAll\n5\n1\n6\n12",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "ch07_da.html",
    "href": "ch07_da.html",
    "title": "7  Discriminant analysis",
    "section": "",
    "text": "7.1 Example 7.4",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Discriminant analysis</span>"
    ]
  },
  {
    "objectID": "ch07_da.html#example-7.4",
    "href": "ch07_da.html#example-7.4",
    "title": "7  Discriminant analysis",
    "section": "",
    "text": "7.1.1 Load data\n\ndat1 = pd.read_csv(\"data/ch7_dat1.csv\")\nX = dat1[['X1', 'X2']]\ny = LabelBinarizer().fit_transform(dat1['class']).ravel()\n\n\n\n7.1.2 Use scikit-learn for LDA\nTraining with priors=[0.5, 0.5]:\n\nlda_model = LinearDiscriminantAnalysis(priors=[0.5, 0.5])\nlda_fit = lda_model.fit(X, y)\n\nIntercept:\n\nlda_fit.intercept_\n\narray([0.79565916])\n\n\nCoefficients:\n\nlda_fit.coef_\n\narray([[ 1.50803859, -1.54180064]])\n\n\nPosterior probability:\n\nlda_fit.predict_proba(X)\n\narray([[0.92105384, 0.07894616],\n       [0.09953414, 0.90046586],\n       [0.72759921, 0.27240079],\n       [0.02636077, 0.97363923],\n       [0.9807542 , 0.0192458 ],\n       [0.98010647, 0.01989353],\n       [0.35592691, 0.64407309],\n       [0.00595706, 0.99404294],\n       [0.10260131, 0.89739869]])\n\n\nClassification:\n\nlda_fit.predict(X)\n\narray([0, 1, 0, 1, 0, 0, 1, 1, 1])",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Discriminant analysis</span>"
    ]
  },
  {
    "objectID": "ch07_da.html#example-7.6",
    "href": "ch07_da.html#example-7.6",
    "title": "7  Discriminant analysis",
    "section": "7.2 Example 7.6",
    "text": "7.2 Example 7.6\n\n7.2.1 Load data\n\ndat1 = pd.read_csv(\"data/ch7_dat1.csv\")\nX = dat1[['X1', 'X2']]\ny = LabelBinarizer().fit_transform(dat1['class']).ravel()\n\n\n\n7.2.2 Use scikit-learn for QDA\nTraining with priors=[0.5, 0.5]:\n\nqda_model = QuadraticDiscriminantAnalysis(priors=[0.5, 0.5], store_covariance=True)\nqda_fit = qda_model.fit(X, y)\n\nVariance-covariance matrix within each class:\n\nqda_fit.covariance_\n\n[array([[3.33333333, 1.        ],\n        [1.        , 0.66666667]]),\n array([[4.3 , 2.95],\n        [2.95, 3.8 ]])]\n\n\nPosterior probabilty:\n\nqda_fit.predict_proba(X)\n\narray([[9.20146229e-01, 7.98537705e-02],\n       [2.85574229e-05, 9.99971443e-01],\n       [3.67555071e-01, 6.32444929e-01],\n       [3.98123298e-02, 9.60187670e-01],\n       [9.91863381e-01, 8.13661871e-03],\n       [9.90954322e-01, 9.04567828e-03],\n       [5.38732649e-01, 4.61267351e-01],\n       [7.21814284e-03, 9.92781857e-01],\n       [2.18482508e-03, 9.97815175e-01]])\n\n\nClassification:\n\nqda_fit.predict(X)\n\narray([0, 1, 1, 1, 0, 0, 0, 1, 1])",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Discriminant analysis</span>"
    ]
  },
  {
    "objectID": "ch07_da.html#example-7.7",
    "href": "ch07_da.html#example-7.7",
    "title": "7  Discriminant analysis",
    "section": "7.3 Example 7.7",
    "text": "7.3 Example 7.7\n\n7.3.1 Load data\n\niris = pd.read_csv(\"data/iris.csv\")\nX = iris[['Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width']]\ny = LabelEncoder().fit_transform(iris['Species']).ravel()\n\n\n\n7.3.2 Train/test split\n\nrandom.seed(478245)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, stratify=y)\n\n\n\n7.3.3 LDA\nTraining:\n\nlda_model = LinearDiscriminantAnalysis()\nlda_fit = lda_model.fit(X_train, y_train)\n\nIntercept:\n\nlda_fit.intercept_\n\narray([-20.39169882,  -1.02788044, -30.57654148])\n\n\nCoefficients:\n\nlda_fit.coef_\n\narray([[ 10.00739616,   9.70177722, -20.67905989, -16.86581712],\n       [ -1.94341141,  -4.3635061 ,   5.15077516,   2.5979871 ],\n       [ -8.06398476,  -5.33827112,  15.52828473,  14.26783001]])\n\n\nPrediction:\n\ny_pred = lda_fit.predict(X_test)\n\nX_test.assign(\n    y = y_test,\n    y_pred = y_pred\n)\n\n\n\n\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\ny\ny_pred\n\n\n\n\n110\n6.5\n3.2\n5.1\n2.0\n2\n2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0\n0\n\n\n7\n5.0\n3.4\n1.5\n0.2\n0\n0\n\n\n141\n6.9\n3.1\n5.1\n2.3\n2\n2\n\n\n130\n7.4\n2.8\n6.1\n1.9\n2\n2\n\n\n6\n4.6\n3.4\n1.4\n0.3\n0\n0\n\n\n34\n4.9\n3.1\n1.5\n0.2\n0\n0\n\n\n99\n5.7\n2.8\n4.1\n1.3\n1\n1\n\n\n30\n4.8\n3.1\n1.6\n0.2\n0\n0\n\n\n90\n5.5\n2.6\n4.4\n1.2\n1\n1\n\n\n14\n5.8\n4.0\n1.2\n0.2\n0\n0\n\n\n100\n6.3\n3.3\n6.0\n2.5\n2\n2\n\n\n50\n7.0\n3.2\n4.7\n1.4\n1\n1\n\n\n124\n6.7\n3.3\n5.7\n2.1\n2\n2\n\n\n28\n5.2\n3.4\n1.4\n0.2\n0\n0\n\n\n113\n5.7\n2.5\n5.0\n2.0\n2\n2\n\n\n15\n5.7\n4.4\n1.5\n0.4\n0\n0\n\n\n61\n5.9\n3.0\n4.2\n1.5\n1\n1\n\n\n85\n6.0\n3.4\n4.5\n1.6\n1\n1\n\n\n45\n4.8\n3.0\n1.4\n0.3\n0\n0\n\n\n38\n4.4\n3.0\n1.3\n0.2\n0\n0\n\n\n55\n5.7\n2.8\n4.5\n1.3\n1\n1\n\n\n77\n6.7\n3.0\n5.0\n1.7\n1\n1\n\n\n97\n6.2\n2.9\n4.3\n1.3\n1\n1\n\n\n93\n5.0\n2.3\n3.3\n1.0\n1\n1\n\n\n119\n6.0\n2.2\n5.0\n1.5\n2\n2\n\n\n27\n5.2\n3.5\n1.5\n0.2\n0\n0\n\n\n95\n5.7\n3.0\n4.2\n1.2\n1\n1\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0\n0\n\n\n142\n5.8\n2.7\n5.1\n1.9\n2\n2\n\n\n98\n5.1\n2.5\n3.0\n1.1\n1\n1\n\n\n104\n6.5\n3.0\n5.8\n2.2\n2\n2\n\n\n5\n5.4\n3.9\n1.7\n0.4\n0\n0\n\n\n59\n5.2\n2.7\n3.9\n1.4\n1\n1\n\n\n108\n6.7\n2.5\n5.8\n1.8\n2\n2\n\n\n94\n5.6\n2.7\n4.2\n1.3\n1\n1\n\n\n96\n5.7\n2.9\n4.2\n1.3\n1\n1\n\n\n13\n4.3\n3.0\n1.1\n0.1\n0\n0\n\n\n127\n6.1\n3.0\n4.9\n1.8\n2\n2\n\n\n147\n6.5\n3.0\n5.2\n2.0\n2\n2\n\n\n111\n6.4\n2.7\n5.3\n1.9\n2\n2\n\n\n24\n4.8\n3.4\n1.9\n0.2\n0\n0\n\n\n114\n5.8\n2.8\n5.1\n2.4\n2\n2\n\n\n35\n5.0\n3.2\n1.2\n0.2\n0\n0\n\n\n101\n5.8\n2.7\n5.1\n1.9\n2\n2\n\n\n87\n6.3\n2.3\n4.4\n1.3\n1\n1\n\n\n131\n7.9\n3.8\n6.4\n2.0\n2\n2\n\n\n138\n6.0\n3.0\n4.8\n1.8\n2\n2\n\n\n37\n4.9\n3.6\n1.4\n0.1\n0\n0\n\n\n58\n6.6\n2.9\n4.6\n1.3\n1\n1\n\n\n74\n6.4\n2.9\n4.3\n1.3\n1\n1\n\n\n66\n5.6\n3.0\n4.5\n1.5\n1\n1\n\n\n23\n5.1\n3.3\n1.7\n0.5\n0\n0\n\n\n79\n5.7\n2.6\n3.5\n1.0\n1\n1\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0\n0\n\n\n144\n6.7\n3.3\n5.7\n2.5\n2\n2\n\n\n84\n5.4\n3.0\n4.5\n1.5\n1\n1\n\n\n122\n7.7\n2.8\n6.7\n2.0\n2\n2\n\n\n123\n6.3\n2.7\n4.9\n1.8\n2\n2\n\n\n17\n5.1\n3.5\n1.4\n0.3\n0\n0\n\n\n\n\n\n\n\n\nConfusion matrix:\n\nconfusion_matrix(y_test, y_pred)\n\narray([[20,  0,  0],\n       [ 0, 20,  0],\n       [ 0,  0, 20]])",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Discriminant analysis</span>"
    ]
  },
  {
    "objectID": "ch08_tree.html",
    "href": "ch08_tree.html",
    "title": "8  Decision tree",
    "section": "",
    "text": "8.1 Examples 8.2 - 8.7",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Decision tree</span>"
    ]
  },
  {
    "objectID": "ch08_tree.html#examples-8.2---8.7",
    "href": "ch08_tree.html#examples-8.2---8.7",
    "title": "8  Decision tree",
    "section": "",
    "text": "8.1.1 Load data\n\ndf_train = pd.read_csv(\"data/ch8_dat1.csv\")\ndf_test = pd.read_csv(\"data/ch8_dat2.csv\")\n\n\nX_train = df_train[['x1', 'x2']]\nX_test = df_test[['x1', 'x2']]\n\n\nlabel = LabelBinarizer()\ny_train = label.fit_transform(df_train['class']).ravel()\ny_test = label.transform(df_test['class']).ravel()\n\n\n\n8.1.2 Ex 8.2: One-level tree\nUse DecisionTreeClassifier from sklearn.tree module to train a classification tree. By default, it uses Gini index as an impurity measurement.\n\ntree_depth1 = DecisionTreeClassifier(max_depth=1)\ntree_depth1.fit(X_train, y_train)\n\nDecisionTreeClassifier(max_depth=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(max_depth=1) \n\n\nVisualize the tree by using plot_tree() from sklearn.tree.\n\nplot_tree(tree_depth1)\n\n[Text(0.5, 0.75, 'x[1] &lt;= 5.5\\ngini = 0.5\\nsamples = 10\\nvalue = [5, 5]'),\n Text(0.25, 0.25, 'gini = 0.408\\nsamples = 7\\nvalue = [2, 5]'),\n Text(0.375, 0.5, 'True  '),\n Text(0.75, 0.25, 'gini = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n Text(0.625, 0.5, '  False')]\n\n\n\n\n\n\n\n\n\n\n\n8.1.3 Ex 8.3: Maximal tree\n\ntree_maximal = DecisionTreeClassifier()\ntree_maximal.fit(X_train, y_train)\nplot_tree(tree_maximal)\n\n[Text(0.5, 0.9, 'x[1] &lt;= 5.5\\ngini = 0.5\\nsamples = 10\\nvalue = [5, 5]'),\n Text(0.3333333333333333, 0.7, 'x[0] &lt;= 1.5\\ngini = 0.408\\nsamples = 7\\nvalue = [2, 5]'),\n Text(0.41666666666666663, 0.8, 'True  '),\n Text(0.16666666666666666, 0.5, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n Text(0.5, 0.5, 'x[1] &lt;= 4.5\\ngini = 0.278\\nsamples = 6\\nvalue = [1, 5]'),\n Text(0.3333333333333333, 0.3, 'gini = 0.0\\nsamples = 4\\nvalue = [0, 4]'),\n Text(0.6666666666666666, 0.3, 'x[0] &lt;= 3.0\\ngini = 0.5\\nsamples = 2\\nvalue = [1, 1]'),\n Text(0.5, 0.1, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n Text(0.8333333333333334, 0.1, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n Text(0.6666666666666666, 0.7, 'gini = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n Text(0.5833333333333333, 0.8, '  False')]\n\n\n\n\n\n\n\n\n\n\n\n8.1.4 Ex 8.6: Pruning\n\npath = tree_maximal.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities\n\n\nprint(ccp_alphas)\n\n[0.         0.08333333 0.11904762 0.21428571]\n\n\n\ntree_prune = DecisionTreeClassifier(ccp_alpha = 0.1)\ntree_prune.fit(X_train, y_train)\nplot_tree(tree_prune)\n\n[Text(0.6, 0.8333333333333334, 'x[1] &lt;= 5.5\\ngini = 0.5\\nsamples = 10\\nvalue = [5, 5]'),\n Text(0.4, 0.5, 'x[0] &lt;= 1.5\\ngini = 0.408\\nsamples = 7\\nvalue = [2, 5]'),\n Text(0.5, 0.6666666666666667, 'True  '),\n Text(0.2, 0.16666666666666666, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n Text(0.6, 0.16666666666666666, 'gini = 0.278\\nsamples = 6\\nvalue = [1, 5]'),\n Text(0.8, 0.5, 'gini = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n Text(0.7, 0.6666666666666667, '  False')]\n\n\n\n\n\n\n\n\n\n\n\n8.1.5 Ex 8.7: Prediction\n\ndf_test.assign(\n  pred_class = label.inverse_transform(tree_prune.predict(X_test))\n)\n\n\n\n\n\n\n\n\n\nx1\nx2\nclass\npred_class\n\n\n\n\n0\n1\n5\n1\n1\n\n\n1\n0\n5\n1\n1\n\n\n2\n3\n4\n2\n2\n\n\n3\n4\n3\n2\n2\n\n\n4\n2\n7\n1\n1\n\n\n5\n1\n4\n2\n1\n\n\n\n\n\n\n\n\nPrediction accuracy\n\naccuracy_score(\n  y_test,\n  tree_prune.predict(X_test)\n)\n\n0.8333333333333334",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Decision tree</span>"
    ]
  },
  {
    "objectID": "ch09_svm.html",
    "href": "ch09_svm.html",
    "title": "9  Support vector machine",
    "section": "",
    "text": "9.1 Example 9.1",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Support vector machine</span>"
    ]
  },
  {
    "objectID": "ch09_svm.html#example-9.1",
    "href": "ch09_svm.html#example-9.1",
    "title": "9  Support vector machine",
    "section": "",
    "text": "9.1.1 Load data\n\ndat = pd.read_csv(\"data/ch9_dat1.csv\")\nX = dat[['x1', 'x2']]\ny = LabelBinarizer(neg_label=-1, pos_label=1).fit_transform(dat['class']).ravel()\n\n\n\n9.1.2 Linear SVM: Separable\n\nsvm_model = SVC(kernel='linear', C=100)\nsvm_model.fit(X, y)\n\nSVC(C=100, kernel='linear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SVC?Documentation for SVCiFittedSVC(C=100, kernel='linear') \n\n\nHyperplane intercept:\n\nsvm_model.intercept_\n\narray([-6.9995])\n\n\nHyperplane coefficients:\n\nsvm_model.coef_\n\narray([[0.6664, 0.6668]])",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Support vector machine</span>"
    ]
  },
  {
    "objectID": "ch09_svm.html#example-9.2",
    "href": "ch09_svm.html#example-9.2",
    "title": "9  Support vector machine",
    "section": "9.2 Example 9.2",
    "text": "9.2 Example 9.2\n\n9.2.1 Load data\n\ndat = pd.read_csv(\"data/ch9_dat2.csv\")\nX = dat[['x1', 'x2']]\ny = LabelBinarizer(neg_label=-1, pos_label=1).fit_transform(dat['class']).ravel()\n\n\n\n9.2.2 Linear SVM: Inseparable\n\nCs = (1, 5, 100)\nsvm_models = [SVC(C=c, kernel='linear') for c in Cs]\nsvm_models = [m.fit(X, y) for m in svm_models]\n\nfor i in range(len(Cs)):\n  print(f\"C = {Cs[i]}, Intercept = {svm_models[i].intercept_}, Coefficients = {svm_models[i].coef_}\")\n\nC = 1, Intercept = [-7.6], Coefficients = [[0.6 0.8]]\nC = 5, Intercept = [-9.39685301], Coefficients = [[0.39987067 1.19961201]]\nC = 100, Intercept = [-9.3975255], Coefficients = [[0.39989831 1.19969492]]",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Support vector machine</span>"
    ]
  },
  {
    "objectID": "ch09_svm.html#example-9.7",
    "href": "ch09_svm.html#example-9.7",
    "title": "9  Support vector machine",
    "section": "9.3 Example 9.7",
    "text": "9.3 Example 9.7\n\n9.3.1 Load data\n\ndat = pd.read_csv(\"data/ch9_dat3.csv\")\nX = dat[['x1', 'x2']]\ny = LabelBinarizer(neg_label=-1, pos_label=1).fit_transform(dat['class']).ravel()\n\n\n\n9.3.2 Nonlinear SVM: Polynomial\nSee kernel function parameterization\n\nCs = (1, 5, 100)\nsvm_models = [SVC(C=c, kernel='poly', degree=2, gamma=1) for c in Cs]\nsvm_models = [m.fit(X, y) for m in svm_models]\n\nfor i in range(len(Cs)):\n  print(f\"C = {Cs[i]}, SV = {svm_models[i].support_}\")\n\nC = 1, SV = [1 2 0 6]\nC = 5, SV = [1 2 0 5 6]\nC = 100, SV = [1 2 0 5 6]",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Support vector machine</span>"
    ]
  },
  {
    "objectID": "ch09_svm.html#example-9.8",
    "href": "ch09_svm.html#example-9.8",
    "title": "9  Support vector machine",
    "section": "9.4 Example 9.8",
    "text": "9.4 Example 9.8\n\n9.4.1 Load data\n\ndat = pd.read_csv(\"data/breast-cancer-wisconsin.csv\").dropna()\nX = dat.iloc[:, 1:10]\ny = LabelBinarizer().fit_transform(dat['class']).ravel()\n\n\n\n9.4.2 Train/test split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state=42724)\n\n\n\n9.4.3 Nonlinear SVM: RBF Kernel with gamma=0.5\nSee kernel function parameterization\n\nsvm_model = SVC(kernel='rbf', C=10, gamma=0.5)\nsvm_model.fit(X_train, y_train)\n\nSVC(C=10, gamma=0.5)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SVC?Documentation for SVCiFittedSVC(C=10, gamma=0.5) \n\n\nNumber of support vectors\n\nsvm_model.n_support_\n\narray([117, 160], dtype=int32)\n\n\n\n9.4.3.1 In-sample prediction\n\nconfusion_matrix(y_train, svm_model.predict(X_train))\n\narray([[294,   0],\n       [  0, 161]])\n\n\n\n\n9.4.3.2 Out-of-sample prediction\n\nconfusion_matrix(y_test, svm_model.predict(X_test))\n\narray([[140,  10],\n       [  0,  78]])\n\n\n\n\n\n9.4.4 Nonlinear SVM: RBF Kernel with gamma=2\nSee kernel function parameterization\n\nsvm_model = SVC(kernel='rbf', C=10, gamma=2)\nsvm_model.fit(X_train, y_train)\n\nSVC(C=10, gamma=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SVC?Documentation for SVCiFittedSVC(C=10, gamma=2) \n\n\nNumber of support vectors\n\nsvm_model.n_support_\n\narray([165, 160], dtype=int32)\n\n\n\n9.4.4.1 In-sample prediction\n\nconfusion_matrix(y_train, svm_model.predict(X_train))\n\narray([[294,   0],\n       [  0, 161]])\n\n\n\n\n9.4.4.2 Out-of-sample prediction\n\nconfusion_matrix(y_test, svm_model.predict(X_test))\n\narray([[128,  22],\n       [  0,  78]])",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Support vector machine</span>"
    ]
  },
  {
    "objectID": "ch10_ensemble.html",
    "href": "ch10_ensemble.html",
    "title": "10  Ensemble",
    "section": "",
    "text": "10.1 Example 10.2 - 10.3",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Ensemble</span>"
    ]
  },
  {
    "objectID": "ch10_ensemble.html#example-10.2---10.3",
    "href": "ch10_ensemble.html#example-10.2---10.3",
    "title": "10  Ensemble",
    "section": "",
    "text": "10.1.1 Load data\n\ndat = pd.read_csv(\"data/ch10_dat1.csv\")\nX = OneHotEncoder().fit_transform(dat[['X1', 'X2', 'X3', 'X4']])\nenc = LabelBinarizer().fit(dat['Y'])\ny = enc.transform(dat['Y']).ravel()\n\n\n\n10.1.2 Ex 10.2: Train random forest classifier\n\nrf = RandomForestClassifier(n_estimators=4, oob_score=True, random_state=3280612)\nrf.fit(X, y)\n\nRandomForestClassifier(n_estimators=4, oob_score=True, random_state=3280612)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(n_estimators=4, oob_score=True, random_state=3280612) \n\n\nVisualize each tree in the random forest.\n\nfor m in rf.estimators_:\n  plot_tree(m)\n  plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.1.3 Ex 10.3: OOB prediction\nOut-of-bag probability prediction:\n\nrf.oob_decision_function_\n\narray([[1.  , 0.  ],\n       [0.  , 1.  ],\n       [0.  , 1.  ],\n       [0.75, 0.25],\n       [0.  , 1.  ]])\n\n\n\ny_oob = np.argmax(rf.oob_decision_function_, axis = 1)\ny_oob\n\narray([0, 1, 1, 0, 1])\n\n\nConfusion matrix on OOB prediction:\n\nconfusion_matrix(y, y_oob)\n\narray([[0, 3],\n       [2, 0]])",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Ensemble</span>"
    ]
  },
  {
    "objectID": "ch10_ensemble.html#example-10.5",
    "href": "ch10_ensemble.html#example-10.5",
    "title": "10  Ensemble",
    "section": "10.2 Example 10.5",
    "text": "10.2 Example 10.5\n\n10.2.1 Load data\n\ndat = pd.read_csv(\"data/ch10_dat3.csv\")\nX = dat[['X']]\ny = dat['Y']\n\n\n\n10.2.2 Estimate GBM\n\ngbm_model = GradientBoostingRegressor(n_estimators=5, max_depth=1, learning_rate=1)\n\n\ngbm_model.fit(X, y)\n\nGradientBoostingRegressor(learning_rate=1, max_depth=1, n_estimators=5)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GradientBoostingRegressor?Documentation for GradientBoostingRegressoriFittedGradientBoostingRegressor(learning_rate=1, max_depth=1, n_estimators=5) \n\n\n\n\n10.2.3 Visualize function\n\nX_new = pd.DataFrame.from_dict({'X': np.arange(start=-5, stop=5, step=0.01)})\ny_pred = gbm_model.predict(X_new)\n\n\nplt.figure()\nplt.scatter(X, y)\nplt.plot(X_new, y_pred, color='orange')\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Ensemble</span>"
    ]
  },
  {
    "objectID": "ch10_ensemble.html#example-10.6",
    "href": "ch10_ensemble.html#example-10.6",
    "title": "10  Ensemble",
    "section": "10.3 Example 10.6",
    "text": "10.3 Example 10.6\n\n10.3.1 Load data\n\ndat = pd.read_csv(\"data/ch8_dat1.csv\")\nX = dat[['x1', 'x2']]\ny = LabelBinarizer().fit_transform(dat['class']).ravel()\n\n\n\n10.3.2 Small GBM Classifier\nLet us use only two trees (i.e. n_estimator = 2)\n\ngbm_model = GradientBoostingClassifier(n_estimators=2, max_depth=1, learning_rate=1)\ngbm_model.fit(X, y)\n\nGradientBoostingClassifier(learning_rate=1, max_depth=1, n_estimators=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GradientBoostingClassifier?Documentation for GradientBoostingClassifieriFittedGradientBoostingClassifier(learning_rate=1, max_depth=1, n_estimators=2) \n\n\n\n\n10.3.3 Prediction\nPosterior:\n\ny_posterior = gbm_model.predict_proba(X)\ny_posterior\n\narray([[0.92409102, 0.07590898],\n       [0.84891394, 0.15108606],\n       [0.24397021, 0.75602979],\n       [0.24397021, 0.75602979],\n       [0.24397021, 0.75602979],\n       [0.84891394, 0.15108606],\n       [0.84891394, 0.15108606],\n       [0.24397021, 0.75602979],\n       [0.24397021, 0.75602979],\n       [0.24397021, 0.75602979]])\n\n\nArea under the ROC curve:\n\nroc_auc_score(y, y_posterior[:, 1])\n\nnp.float64(0.9)\n\n\n\n\n10.3.4 Larger model\nLet us increase number of trees, while reducing step size:\n\ngbm_model = GradientBoostingClassifier(n_estimators=100, max_depth=1, learning_rate=0.1)\ngbm_model.fit(X, y)\n\nGradientBoostingClassifier(max_depth=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GradientBoostingClassifier?Documentation for GradientBoostingClassifieriFittedGradientBoostingClassifier(max_depth=1) \n\n\nPosterior:\n\ny_posterior = gbm_model.predict_proba(X)\ny_posterior\n\narray([[0.90889146, 0.09110854],\n       [0.98989577, 0.01010423],\n       [0.76814463, 0.23185537],\n       [0.11838389, 0.88161611],\n       [0.08298002, 0.91701998],\n       [0.98989577, 0.01010423],\n       [0.89455077, 0.10544923],\n       [0.22292706, 0.77707294],\n       [0.0114939 , 0.9885061 ],\n       [0.0114939 , 0.9885061 ]])\n\n\nAUC:\n\nroc_auc_score(y, y_posterior[:, 1])\n\nnp.float64(1.0)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Ensemble</span>"
    ]
  },
  {
    "objectID": "ch11_cluster.html",
    "href": "ch11_cluster.html",
    "title": "11  Cluster analysis",
    "section": "",
    "text": "11.1 Examples 11.1, 11.3",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Cluster analysis</span>"
    ]
  },
  {
    "objectID": "ch11_cluster.html#examples-11.1-11.3",
    "href": "ch11_cluster.html#examples-11.1-11.3",
    "title": "11  Cluster analysis",
    "section": "",
    "text": "11.1.1 Load data\n\ndat1 = pd.read_csv(\"data/ch11_dat1.csv\")\nX = dat1[['X1', 'X2', 'X3']]\n\n\n\n11.1.2 Ex 11.1\n\n11.1.2.1 Euclidean distance\n\nD1 = pairwise.euclidean_distances(X)\nnp.round(D1, 2)\n\narray([[ 0.  ,  8.31, 24.74, 16.91, 15.17, 10.1 , 27.17, 27.87, 33.03,\n        24.37],\n       [ 8.31,  0.  , 16.76,  9.11,  9.43,  3.  , 19.05, 20.35, 25.57,\n        17.75],\n       [24.74, 16.76,  0.  ,  7.87, 12.08, 16.55,  3.16, 10.05, 14.53,\n        11.75],\n       [16.91,  9.11,  7.87,  0.  ,  6.48,  9.49, 10.39, 13.15, 18.57,\n        11.66],\n       [15.17,  9.43, 12.08,  6.48,  0.  , 11.31, 15.17, 19.1 , 24.52,\n        17.03],\n       [10.1 ,  3.  , 16.55,  9.49, 11.31,  0.  , 18.49, 19.52, 24.52,\n        17.49],\n       [27.17, 19.05,  3.16, 10.39, 15.17, 18.49,  0.  ,  8.31, 12.04,\n        11.49],\n       [27.87, 20.35, 10.05, 13.15, 19.1 , 19.52,  8.31,  0.  ,  5.48,\n         5.74],\n       [33.03, 25.57, 14.53, 18.57, 24.52, 24.52, 12.04,  5.48,  0.  ,\n        10.05],\n       [24.37, 17.75, 11.75, 11.66, 17.03, 17.49, 11.49,  5.74, 10.05,\n         0.  ]])\n\n\nDistance between 2nd object and 4th object. Please note that python’s position index start from 0, so you should use positions 1 and 3.\n\nD1[1, 3]\n\nnp.float64(9.1104335791443)\n\n\n\n\n11.1.2.2 Standardized Euclidean distance\nStandardize data by using StandardScaler from sklearn.preprocessing module.\n\nstd_X = StandardScaler().fit_transform(X)\nstd_D1 = pairwise.euclidean_distances(std_X)\nnp.round(std_D1, 2)\n\narray([[0.  , 1.01, 3.53, 2.42, 2.86, 1.13, 3.65, 3.62, 4.24, 3.61],\n       [1.01, 0.  , 2.61, 1.53, 2.15, 0.55, 2.7 , 2.9 , 3.56, 3.09],\n       [3.53, 2.61, 0.  , 1.13, 1.34, 2.92, 0.4 , 2.21, 2.85, 2.74],\n       [2.42, 1.53, 1.13, 0.  , 1.11, 1.92, 1.26, 2.06, 2.8 , 2.41],\n       [2.86, 2.15, 1.34, 1.11, 0.  , 2.61, 1.71, 3.  , 3.74, 3.27],\n       [1.13, 0.55, 2.92, 1.92, 2.61, 0.  , 2.96, 3.07, 3.66, 3.31],\n       [3.65, 2.7 , 0.4 , 1.26, 1.71, 2.96, 0.  , 1.94, 2.53, 2.56],\n       [3.62, 2.9 , 2.21, 2.06, 3.  , 3.07, 1.94, 0.  , 0.75, 0.85],\n       [4.24, 3.56, 2.85, 2.8 , 3.74, 3.66, 2.53, 0.75, 0.  , 1.11],\n       [3.61, 3.09, 2.74, 2.41, 3.27, 3.31, 2.56, 0.85, 1.11, 0.  ]])\n\n\n\n\n\n\n\n\nNote\n\n\n\nStandardScaler uses population variance formula instead of sample variance formula, so the results are slightly different from book example.\n\n\nStandardized distance between 2nd object and 4th object.\n\nstd_D1[1, 3]\n\nnp.float64(1.5332230530992101)\n\n\n\n\n\n11.1.3 Ex 11.3\n\n11.1.3.1 Correlation coefficients\n\nrow_cor = np.corrcoef(X)\nrow_cor.round(2)\n\narray([[1.  , 0.93, 0.68, 0.76, 0.58, 0.97, 0.71, 0.81, 0.82, 0.81],\n       [0.93, 1.  , 0.9 , 0.94, 0.83, 0.99, 0.92, 0.97, 0.97, 0.97],\n       [0.68, 0.9 , 1.  , 0.99, 0.99, 0.85, 1.  , 0.98, 0.98, 0.98],\n       [0.76, 0.94, 0.99, 1.  , 0.97, 0.9 , 1.  , 1.  , 0.99, 1.  ],\n       [0.58, 0.83, 0.99, 0.97, 1.  , 0.77, 0.98, 0.95, 0.94, 0.95],\n       [0.97, 0.99, 0.85, 0.9 , 0.77, 1.  , 0.87, 0.93, 0.94, 0.93],\n       [0.71, 0.92, 1.  , 1.  , 0.98, 0.87, 1.  , 0.99, 0.99, 0.99],\n       [0.81, 0.97, 0.98, 1.  , 0.95, 0.93, 0.99, 1.  , 1.  , 1.  ],\n       [0.82, 0.97, 0.98, 0.99, 0.94, 0.94, 0.99, 1.  , 1.  , 1.  ],\n       [0.81, 0.97, 0.98, 1.  , 0.95, 0.93, 0.99, 1.  , 1.  , 1.  ]])\n\n\nCorrelation coefficients between 1st object and 8th object.\n\nrow_cor[0, 7]\n\nnp.float64(0.8099343377536524)\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can also use cosine similarity after centering by row.\n\ncentered_X = X.to_numpy() - np.mean(X, axis=1).to_numpy().reshape(-1, 1)\npairwise.cosine_similarity(centered_X).round(2)\n\narray([[1.  , 0.93, 0.68, 0.76, 0.58, 0.97, 0.71, 0.81, 0.82, 0.81],\n       [0.93, 1.  , 0.9 , 0.94, 0.83, 0.99, 0.92, 0.97, 0.97, 0.97],\n       [0.68, 0.9 , 1.  , 0.99, 0.99, 0.85, 1.  , 0.98, 0.98, 0.98],\n       [0.76, 0.94, 0.99, 1.  , 0.97, 0.9 , 1.  , 1.  , 0.99, 1.  ],\n       [0.58, 0.83, 0.99, 0.97, 1.  , 0.77, 0.98, 0.95, 0.94, 0.95],\n       [0.97, 0.99, 0.85, 0.9 , 0.77, 1.  , 0.87, 0.93, 0.94, 0.93],\n       [0.71, 0.92, 1.  , 1.  , 0.98, 0.87, 1.  , 0.99, 0.99, 0.99],\n       [0.81, 0.97, 0.98, 1.  , 0.95, 0.93, 0.99, 1.  , 1.  , 1.  ],\n       [0.82, 0.97, 0.98, 0.99, 0.94, 0.94, 0.99, 1.  , 1.  , 1.  ],\n       [0.81, 0.97, 0.98, 1.  , 0.95, 0.93, 0.99, 1.  , 1.  , 1.  ]])\n\n\n\n\n\n\n11.1.3.2 After standardization\n\nstd_row_cor = np.corrcoef(std_X)\nstd_row_cor.round(2)\n\narray([[ 1.  ,  1.  , -0.67,  0.17,  0.1 ,  0.97, -0.88, -0.8 , -0.75,\n        -0.8 ],\n       [ 1.  ,  1.  , -0.67,  0.18,  0.11,  0.97, -0.87, -0.81, -0.76,\n        -0.8 ],\n       [-0.67, -0.67,  1.  ,  0.62,  0.67, -0.83,  0.94,  0.1 ,  0.01,\n         0.09],\n       [ 0.17,  0.18,  0.62,  1.  ,  1.  , -0.07,  0.32, -0.72, -0.78,\n        -0.73],\n       [ 0.1 ,  0.11,  0.67,  1.  ,  1.  , -0.13,  0.38, -0.68, -0.74,\n        -0.68],\n       [ 0.97,  0.97, -0.83, -0.07, -0.13,  1.  , -0.97, -0.64, -0.57,\n        -0.63],\n       [-0.88, -0.87,  0.94,  0.32,  0.38, -0.97,  1.  ,  0.42,  0.34,\n         0.41],\n       [-0.8 , -0.81,  0.1 , -0.72, -0.68, -0.64,  0.42,  1.  ,  1.  ,\n         1.  ],\n       [-0.75, -0.76,  0.01, -0.78, -0.74, -0.57,  0.34,  1.  ,  1.  ,\n         1.  ],\n       [-0.8 , -0.8 ,  0.09, -0.73, -0.68, -0.63,  0.41,  1.  ,  1.  ,\n         1.  ]])\n\n\nCorrelation coefficients between 1st object and 8th object after standardization.\n\nstd_row_cor[0, 7]\n\nnp.float64(-0.8006555578549494)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Cluster analysis</span>"
    ]
  },
  {
    "objectID": "ch11_cluster.html#example-11.4",
    "href": "ch11_cluster.html#example-11.4",
    "title": "11  Cluster analysis",
    "section": "11.2 Example 11.4",
    "text": "11.2 Example 11.4\n\n11.2.1 Load data\n\ndf = pd.read_csv(\"data/ch11_dat2.csv\")\ndf\n\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\nx5\n\n\n\n\n0\n1\n1\n1\n0\n1\n\n\n1\n1\n0\n1\n0\n0\n\n\n2\n0\n1\n0\n1\n0\n\n\n\n\n\n\n\n\n\n\n11.2.2 Jaccard index\n\njaccard_sim = np.eye(len(df))\nfor i in range(len(df) - 1):\n  for j in range(i + 1, len(df)):\n    jaccard_sim[i, j] = jaccard_score(df.loc[i], df.loc[j])\n    jaccard_sim[j, i] = jaccard_sim[i, j]\n\njaccard_sim\n\narray([[1. , 0.5, 0.2],\n       [0.5, 1. , 0. ],\n       [0.2, 0. , 1. ]])",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Cluster analysis</span>"
    ]
  },
  {
    "objectID": "ch12_hierarchical_clustering.html",
    "href": "ch12_hierarchical_clustering.html",
    "title": "12  Hierarchical clustering",
    "section": "",
    "text": "12.1 Example 12.1",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Hierarchical clustering</span>"
    ]
  },
  {
    "objectID": "ch12_hierarchical_clustering.html#example-12.1",
    "href": "ch12_hierarchical_clustering.html#example-12.1",
    "title": "12  Hierarchical clustering",
    "section": "",
    "text": "12.1.1 Load data\n\ndat1 = pd.read_csv(\"data/ch12_dat1.csv\")\nX = dat1[['X1', 'X2']]\n\n\n\n12.1.2 Average linkage\nCreate AgglomerativeClustering object with specifying number of clusters n_clusters and linkage method linkage.\n\nhc_c = AgglomerativeClustering(n_clusters=3, linkage='average')\nhc_c.fit(X)\n\nAgglomerativeClustering(linkage='average', n_clusters=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  AgglomerativeClustering?Documentation for AgglomerativeClusteringiFittedAgglomerativeClustering(linkage='average', n_clusters=3) \n\n\nlabels_ attribute stores cluster label for each observation.\n\ndat1.assign(\n  cluster = hc_c.labels_\n)\n\n\n\n\n\n\n\n\n\nID\nX1\nX2\ncluster\n\n\n\n\n0\n1\n6\n14\n2\n\n\n1\n2\n8\n13\n2\n\n\n2\n3\n14\n6\n0\n\n\n3\n4\n11\n8\n0\n\n\n4\n5\n15\n7\n0\n\n\n5\n6\n7\n15\n2\n\n\n6\n7\n13\n6\n0\n\n\n7\n8\n5\n4\n1\n\n\n8\n9\n3\n3\n1\n\n\n9\n10\n3\n2\n1",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Hierarchical clustering</span>"
    ]
  },
  {
    "objectID": "ch12_hierarchical_clustering.html#example-12.2",
    "href": "ch12_hierarchical_clustering.html#example-12.2",
    "title": "12  Hierarchical clustering",
    "section": "12.2 Example 12.2",
    "text": "12.2 Example 12.2\n\n12.2.1 Load data\n\ndat2 = pd.read_csv(\"data/ch12_dat2.csv\")\nX = dat2.drop('ID', axis=1)\n\n\n\n12.2.2 Ward’s method\n\nhc_c = AgglomerativeClustering(n_clusters=3, linkage='ward')\nhc_c.fit(X)\n\nAgglomerativeClustering(n_clusters=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  AgglomerativeClustering?Documentation for AgglomerativeClusteringiFittedAgglomerativeClustering(n_clusters=3) \n\n\n\ndat2.assign(\n  cluster = hc_c.labels_\n)\n\n\n\n\n\n\n\n\n\nID\nx1\nx2\ncluster\n\n\n\n\n0\n1\n4\n15\n0\n\n\n1\n2\n20\n13\n1\n\n\n2\n3\n3\n13\n0\n\n\n3\n4\n19\n4\n2\n\n\n4\n5\n17\n17\n1\n\n\n5\n6\n8\n11\n0\n\n\n6\n7\n19\n12\n1\n\n\n7\n8\n18\n6\n2",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Hierarchical clustering</span>"
    ]
  },
  {
    "objectID": "ch12_hierarchical_clustering.html#example-12.5",
    "href": "ch12_hierarchical_clustering.html#example-12.5",
    "title": "12  Hierarchical clustering",
    "section": "12.3 Example 12.5",
    "text": "12.3 Example 12.5\n\n12.3.1 Load data\n\ndat2 = pd.read_csv(\"data/ch12_dat2.csv\")\nX = dat2.drop('ID', axis=1)\n\n\n\n12.3.2 Average silhouette width\nCompute average silhouette width by number of clusters. Use silhouette_score() from sklearn.metrics module.\n\nn_clusters = np.arange(start=2, stop=7)\navg_silhouette = list()\nfor k in n_clusters:\n  hc_c = AgglomerativeClustering(n_clusters=k, linkage='ward')\n  hc_c.fit(X)\n  avg_silhouette.append(silhouette_score(X, hc_c.labels_))\n\nVisualize average silhouette width by number of clusters.\n\nplt.figure()\nplt.scatter(\n  x=n_clusters,\n  y=avg_silhouette\n)\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Hierarchical clustering</span>"
    ]
  },
  {
    "objectID": "ch13_nonhierarchical_clustering.html",
    "href": "ch13_nonhierarchical_clustering.html",
    "title": "13  Non-hierarchical clustering",
    "section": "",
    "text": "13.1 Example 13.1",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Non-hierarchical clustering</span>"
    ]
  },
  {
    "objectID": "ch13_nonhierarchical_clustering.html#example-13.1",
    "href": "ch13_nonhierarchical_clustering.html#example-13.1",
    "title": "13  Non-hierarchical clustering",
    "section": "",
    "text": "13.1.1 Load data\n\ndat1 = pd.read_csv(\"data/ch12_dat1.csv\")\nX = dat1.drop('ID', axis=1)\n\n\n\n13.1.2 K-means\n\nkm = KMeans(n_clusters=3)\nkm.fit(X)\n\nKMeans(n_clusters=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=3) \n\n\nCluster centers:\n\nkm.cluster_centers_\n\narray([[ 7.        , 14.        ],\n       [13.25      ,  6.75      ],\n       [ 3.66666667,  3.        ]])\n\n\nCluster labels:\n\ndat1.assign(\n  cluster = km.labels_\n)\n\n\n\n\n\n\n\n\n\nID\nX1\nX2\ncluster\n\n\n\n\n0\n1\n6\n14\n0\n\n\n1\n2\n8\n13\n0\n\n\n2\n3\n14\n6\n1\n\n\n3\n4\n11\n8\n1\n\n\n4\n5\n15\n7\n1\n\n\n5\n6\n7\n15\n0\n\n\n6\n7\n13\n6\n1\n\n\n7\n8\n5\n4\n2\n\n\n8\n9\n3\n3\n2\n\n\n9\n10\n3\n2\n2",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Non-hierarchical clustering</span>"
    ]
  },
  {
    "objectID": "ch13_nonhierarchical_clustering.html#example-13.8",
    "href": "ch13_nonhierarchical_clustering.html#example-13.8",
    "title": "13  Non-hierarchical clustering",
    "section": "13.2 Example 13.8",
    "text": "13.2 Example 13.8\n\n13.2.1 Load data\n\ndat1 = pd.read_csv(\"data/ch12_dat1.csv\")\nX = dat1.drop('ID', axis=1)\n\n\n\n13.2.2 DBSCAN\n\ndb = DBSCAN(eps=2.5, min_samples=3)\ndb.fit(X)\n\nDBSCAN(eps=2.5, min_samples=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DBSCAN?Documentation for DBSCANiFittedDBSCAN(eps=2.5, min_samples=3) \n\n\nCore samples:\n\ndb.core_sample_indices_\n\narray([0, 1, 2, 4, 5, 6, 8])\n\n\nCluster labels:\n\ndat1.assign(\n  cluster = db.labels_\n)\n\n\n\n\n\n\n\n\n\nID\nX1\nX2\ncluster\n\n\n\n\n0\n1\n6\n14\n0\n\n\n1\n2\n8\n13\n0\n\n\n2\n3\n14\n6\n1\n\n\n3\n4\n11\n8\n-1\n\n\n4\n5\n15\n7\n1\n\n\n5\n6\n7\n15\n0\n\n\n6\n7\n13\n6\n1\n\n\n7\n8\n5\n4\n2\n\n\n8\n9\n3\n3\n2\n\n\n9\n10\n3\n2\n2",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Non-hierarchical clustering</span>"
    ]
  },
  {
    "objectID": "ch14_cluster_validation.html",
    "href": "ch14_cluster_validation.html",
    "title": "14  Cluster validation",
    "section": "",
    "text": "14.1 Examples 14.1, 14.3",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Cluster validation</span>"
    ]
  },
  {
    "objectID": "ch14_cluster_validation.html#examples-14.1-14.3",
    "href": "ch14_cluster_validation.html#examples-14.1-14.3",
    "title": "14  Cluster validation",
    "section": "",
    "text": "14.1.1 Data\n\nsol1 = np.array([1, 1, 2, 2, 2, 3, 3])\nsol2 = np.array([1, 1, 2, 3, 2, 1, 3])\n\n\n\n14.1.2 Ex 14.1 Rand index\n\nrand_score(sol1, sol2)\n\nnp.float64(0.7142857142857143)\n\n\n\n\n14.1.3 Ex 14.3 Adjusted Rand index\n\nadjusted_rand_score(sol1, sol2)\n\n0.2125",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Cluster validation</span>"
    ]
  },
  {
    "objectID": "ch14_cluster_validation.html#example-14.4",
    "href": "ch14_cluster_validation.html#example-14.4",
    "title": "14  Cluster validation",
    "section": "14.2 Example 14.4",
    "text": "14.2 Example 14.4\n\n14.2.1 Data\n\nsol1 = np.array([1, 1, 1, 1, 2, 2, 2, 3, 3, 3])\nsol2 = np.array([1, 1, 2, 3, 1, 2, 3, 1, 2, 3])\n\n\n\n14.2.2 Rand index\n\nrand_score(sol1, sol2)\n\nnp.float64(0.5111111111111111)\n\n\n\n\n14.2.3 Adjusted Rand index\n\nadjusted_rand_score(sol1, sol2)\n\n-0.25",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Cluster validation</span>"
    ]
  },
  {
    "objectID": "ch14_cluster_validation.html#example-14.5",
    "href": "ch14_cluster_validation.html#example-14.5",
    "title": "14  Cluster validation",
    "section": "14.3 Example 14.5",
    "text": "14.3 Example 14.5\n\n14.3.1 Load data\n\ndf = pd.read_csv(\"data/ch14_dat1.csv\")\n\n\n\n14.3.2 Cluster solutions\n\nsol1 = np.array([1, 2, 1, 3, 2, 1, 2, 3])\nsol2 = np.array([1, 2, 1, 2, 2, 1, 2, 2])\n\n\n\n14.3.3 Calinski-Harabasz index\n\ncalinski_harabasz_score(df, sol1)\n\nnp.float64(26.450289575289577)\n\n\n\ncalinski_harabasz_score(df, sol2)\n\nnp.float64(15.362179487179493)\n\n\n\n\n14.3.4 Average silhouette width\n\nsilhouette_score(df, sol1)\n\nnp.float64(0.6507364311718348)\n\n\n\nsilhouette_score(df, sol2)\n\nnp.float64(0.5864225849318143)\n\n\n\n\n14.3.5 Silhouette for each sample\n\ndf.assign(\n  silhouetee = silhouette_samples(df, sol1)\n)\n\n\n\n\n\n\n\n\n\nx1\nx2\nsilhouetee\n\n\n\n\n0\n4\n15\n0.734391\n\n\n1\n20\n13\n0.607345\n\n\n2\n3\n13\n0.759792\n\n\n3\n19\n4\n0.777935\n\n\n4\n17\n17\n0.570831\n\n\n5\n8\n11\n0.513246\n\n\n6\n19\n12\n0.517184\n\n\n7\n18\n6\n0.725166\n\n\n\n\n\n\n\n\n\ndf.assign(\n  silhouetee = silhouette_samples(df, sol2)\n)\n\n\n\n\n\n\n\n\n\nx1\nx2\nsilhouetee\n\n\n\n\n0\n4\n15\n0.752787\n\n\n1\n20\n13\n0.623266\n\n\n2\n3\n13\n0.769059\n\n\n3\n19\n4\n0.513305\n\n\n4\n17\n17\n0.326817\n\n\n5\n8\n11\n0.526063\n\n\n6\n19\n12\n0.630393\n\n\n7\n18\n6\n0.549690",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Cluster validation</span>"
    ]
  },
  {
    "objectID": "ch15_association_rule.html",
    "href": "ch15_association_rule.html",
    "title": "15  Association rules",
    "section": "",
    "text": "15.1 Examples 15.3 - 15.4",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Association rules</span>"
    ]
  },
  {
    "objectID": "ch15_association_rule.html#examples-15.3---15.4",
    "href": "ch15_association_rule.html#examples-15.3---15.4",
    "title": "15  Association rules",
    "section": "",
    "text": "15.1.1 Load data\n\ndf = pd.read_csv(\"data/ch15_transaction.csv\")\nX = df.groupby('id')['item'].apply(list)\nX\n\nid\n1          [b, c, g]\n2    [a, b, d, e, f]\n3       [a, b, c, g]\n4       [b, c, e, f]\n5    [b, c, e, f, g]\nName: item, dtype: object\n\n\n\n\n15.1.2 Encode transaction data\nTransform a list of transactions (i.e. a list of list of items) to a data frame that a row represents a transaction and a column represents an item.\n\nenc = TransactionEncoder()\nX_transformed = enc.fit_transform(X)\ndf_transformed = pd.DataFrame(X_transformed, columns=enc.columns_)\ndf_transformed\n\n\n\n\n\n\n\n\n\na\nb\nc\nd\ne\nf\ng\n\n\n\n\n0\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n1\nTrue\nTrue\nFalse\nTrue\nTrue\nTrue\nFalse\n\n\n2\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n3\nFalse\nTrue\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\nFalse\nTrue\nTrue\nFalse\nTrue\nTrue\nTrue\n\n\n\n\n\n\n\n\n\n\n15.1.3 Ex 15.3: Frequent items\nUse apriori() from mlxtend.frequent_patterns module.\n\nfrequent_items = apriori(df_transformed, min_support=0.4, use_colnames=True)\nfrequent_items\n\n\n\n\n\n\n\n\n\nsupport\nitemsets\n\n\n\n\n0\n0.4\n(a)\n\n\n1\n1.0\n(b)\n\n\n2\n0.8\n(c)\n\n\n3\n0.6\n(e)\n\n\n4\n0.6\n(f)\n\n\n5\n0.6\n(g)\n\n\n6\n0.4\n(b, a)\n\n\n7\n0.8\n(b, c)\n\n\n8\n0.6\n(e, b)\n\n\n9\n0.6\n(b, f)\n\n\n10\n0.6\n(b, g)\n\n\n11\n0.4\n(e, c)\n\n\n12\n0.4\n(c, f)\n\n\n13\n0.6\n(g, c)\n\n\n14\n0.6\n(e, f)\n\n\n15\n0.4\n(e, b, c)\n\n\n16\n0.4\n(b, c, f)\n\n\n17\n0.6\n(g, b, c)\n\n\n18\n0.6\n(e, b, f)\n\n\n19\n0.4\n(e, c, f)\n\n\n20\n0.4\n(e, b, c, f)\n\n\n\n\n\n\n\n\n\n\n15.1.4 Ex 15.4: Association rules\nUse association_rules() from mlxtend.frequent_patterns module.\n\nrules = association_rules(frequent_items, metric='confidence', min_threshold=0.7)\nrules\n\n\n\n\n\n\n\n\n\nantecedents\nconsequents\nantecedent support\nconsequent support\nsupport\nconfidence\nlift\nleverage\nconviction\nzhangs_metric\n\n\n\n\n0\n(a)\n(b)\n0.4\n1.0\n0.4\n1.00\n1.000000\n0.00\ninf\n0.000000\n\n\n1\n(b)\n(c)\n1.0\n0.8\n0.8\n0.80\n1.000000\n0.00\n1.0\n0.000000\n\n\n2\n(c)\n(b)\n0.8\n1.0\n0.8\n1.00\n1.000000\n0.00\ninf\n0.000000\n\n\n3\n(e)\n(b)\n0.6\n1.0\n0.6\n1.00\n1.000000\n0.00\ninf\n0.000000\n\n\n4\n(f)\n(b)\n0.6\n1.0\n0.6\n1.00\n1.000000\n0.00\ninf\n0.000000\n\n\n5\n(g)\n(b)\n0.6\n1.0\n0.6\n1.00\n1.000000\n0.00\ninf\n0.000000\n\n\n6\n(g)\n(c)\n0.6\n0.8\n0.6\n1.00\n1.250000\n0.12\ninf\n0.500000\n\n\n7\n(c)\n(g)\n0.8\n0.6\n0.6\n0.75\n1.250000\n0.12\n1.6\n1.000000\n\n\n8\n(e)\n(f)\n0.6\n0.6\n0.6\n1.00\n1.666667\n0.24\ninf\n1.000000\n\n\n9\n(f)\n(e)\n0.6\n0.6\n0.6\n1.00\n1.666667\n0.24\ninf\n1.000000\n\n\n10\n(e, c)\n(b)\n0.4\n1.0\n0.4\n1.00\n1.000000\n0.00\ninf\n0.000000\n\n\n11\n(c, f)\n(b)\n0.4\n1.0\n0.4\n1.00\n1.000000\n0.00\ninf\n0.000000\n\n\n12\n(b, g)\n(c)\n0.6\n0.8\n0.6\n1.00\n1.250000\n0.12\ninf\n0.500000\n\n\n13\n(c, g)\n(b)\n0.6\n1.0\n0.6\n1.00\n1.000000\n0.00\ninf\n0.000000\n\n\n14\n(b, c)\n(g)\n0.8\n0.6\n0.6\n0.75\n1.250000\n0.12\n1.6\n1.000000\n\n\n15\n(g)\n(b, c)\n0.6\n0.8\n0.6\n1.00\n1.250000\n0.12\ninf\n0.500000\n\n\n16\n(c)\n(b, g)\n0.8\n0.6\n0.6\n0.75\n1.250000\n0.12\n1.6\n1.000000\n\n\n17\n(e, b)\n(f)\n0.6\n0.6\n0.6\n1.00\n1.666667\n0.24\ninf\n1.000000\n\n\n18\n(e, f)\n(b)\n0.6\n1.0\n0.6\n1.00\n1.000000\n0.00\ninf\n0.000000\n\n\n19\n(b, f)\n(e)\n0.6\n0.6\n0.6\n1.00\n1.666667\n0.24\ninf\n1.000000\n\n\n20\n(e)\n(b, f)\n0.6\n0.6\n0.6\n1.00\n1.666667\n0.24\ninf\n1.000000\n\n\n21\n(f)\n(e, b)\n0.6\n0.6\n0.6\n1.00\n1.666667\n0.24\ninf\n1.000000\n\n\n22\n(e, c)\n(f)\n0.4\n0.6\n0.4\n1.00\n1.666667\n0.16\ninf\n0.666667\n\n\n23\n(c, f)\n(e)\n0.4\n0.6\n0.4\n1.00\n1.666667\n0.16\ninf\n0.666667\n\n\n24\n(e, b, c)\n(f)\n0.4\n0.6\n0.4\n1.00\n1.666667\n0.16\ninf\n0.666667\n\n\n25\n(e, c, f)\n(b)\n0.4\n1.0\n0.4\n1.00\n1.000000\n0.00\ninf\n0.000000\n\n\n26\n(b, c, f)\n(e)\n0.4\n0.6\n0.4\n1.00\n1.666667\n0.16\ninf\n0.666667\n\n\n27\n(e, c)\n(b, f)\n0.4\n0.6\n0.4\n1.00\n1.666667\n0.16\ninf\n0.666667\n\n\n28\n(c, f)\n(e, b)\n0.4\n0.6\n0.4\n1.00\n1.666667\n0.16\ninf\n0.666667",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Association rules</span>"
    ]
  }
]