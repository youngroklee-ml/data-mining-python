[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Mining with Python",
    "section": "",
    "text": "1 Preface\nThis is a book to convert example code on https://youngroklee-ml.github.io/data-mining-techniques/ into Python.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "ch02_regression.html",
    "href": "ch02_regression.html",
    "title": "2  Regression",
    "section": "",
    "text": "2.1 Examples 2.3 - 2.5, 2.7, 2.10 - 2.11",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "ch02_regression.html#examples-2.3---2.5-2.7-2.10---2.11",
    "href": "ch02_regression.html#examples-2.3---2.5-2.7-2.10---2.11",
    "title": "2  Regression",
    "section": "",
    "text": "2.1.1 Load data\n\ndat1 = pd.read_csv(\"data/ch2_reg1.csv\")\n\n# print data\ndat1\n\n\n\n\n\n\n\n\n\nID\nage\nheight\nweight\n\n\n\n\n0\n1\n21\n170\n60\n\n\n1\n2\n47\n167\n65\n\n\n2\n3\n36\n173\n67\n\n\n3\n4\n15\n165\n54\n\n\n4\n5\n54\n168\n73\n\n\n5\n6\n25\n177\n71\n\n\n6\n7\n32\n169\n68\n\n\n7\n8\n18\n172\n62\n\n\n8\n9\n43\n171\n66\n\n\n9\n10\n28\n175\n68\n\n\n\n\n\n\n\n\n\n\n2.1.2 Ex 2.3: Regression coefficient\nDefine a linear regression model with smf.ols() by passing formula string and training data as arguments.\n\nmodel = smf.ols(\"weight ~ age + height\", data = dat1)\n\nEstimate the model by calling fit() method.\n\nmodel_fit = model.fit()\n\nLet’s see a summary of model estimation results.\n\nmodel_fit.summary()\n\n/opt/hostedtoolcache/Python/3.11.9/x64/lib/python3.11/site-packages/scipy/stats/_axis_nan_policy.py:531: UserWarning: kurtosistest only valid for n&gt;=20 ... continuing anyway, n=10\n  res = hypotest_fun_out(*samples, **kwds)\n\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nweight\nR-squared:\n0.822\n\n\nModel:\nOLS\nAdj. R-squared:\n0.771\n\n\nMethod:\nLeast Squares\nF-statistic:\n16.13\n\n\nDate:\nSun, 30 Jun 2024\nProb (F-statistic):\n0.00239\n\n\nTime:\n19:13:53\nLog-Likelihood:\n-22.163\n\n\nNo. Observations:\n10\nAIC:\n50.33\n\n\nDf Residuals:\n7\nBIC:\n51.23\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-108.1672\n42.123\n-2.568\n0.037\n-207.772\n-8.563\n\n\nage\n0.3291\n0.069\n4.753\n0.002\n0.165\n0.493\n\n\nheight\n0.9553\n0.244\n3.914\n0.006\n0.378\n1.532\n\n\n\n\n\n\nOmnibus:\n0.731\nDurbin-Watson:\n1.196\n\n\nProb(Omnibus):\n0.694\nJarque-Bera (JB):\n0.620\n\n\nSkew:\n0.477\nProb(JB):\n0.733\n\n\nKurtosis:\n2.239\nCond. No.\n8.72e+03\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 8.72e+03. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\nCoefficient estimates can be accessed with params attribute.\n\nmodel_fit.params\n\nIntercept   -108.167199\nage            0.329121\nheight         0.955291\ndtype: float64\n\n\n\n\n2.1.3 Ex 2.4: Variance of error terms\nscale attribute represents the estimate of error variance.\n\nmodel_fit.scale\n\nnp.float64(7.038464456795462)\n\n\n\n\n2.1.4 Ex 2.5: Test a model\nUse anova_lm() from statsmodels.api.stats module and pass the fitted linear regression model to conduct ANOVA test.\n\nsm.stats.anova_lm(model_fit)\n\n\n\n\n\n\n\n\n\ndf\nsum_sq\nmean_sq\nF\nPR(&gt;F)\n\n\n\n\nage\n1.0\n119.299334\n119.299334\n16.949625\n0.004476\n\n\nheight\n1.0\n107.831415\n107.831415\n15.320304\n0.005793\n\n\nResidual\n7.0\n49.269251\n7.038464\nNaN\nNaN\n\n\n\n\n\n\n\n\n\n\n2.1.5 Ex 2.7 Test regression coefficients\nOne of tables from summary() output represents coefficient-level statistics including t-test statstics.\n\nmodel_fit.summary().tables[1]\n\n/opt/hostedtoolcache/Python/3.11.9/x64/lib/python3.11/site-packages/scipy/stats/_axis_nan_policy.py:531: UserWarning: kurtosistest only valid for n&gt;=20 ... continuing anyway, n=10\n  res = hypotest_fun_out(*samples, **kwds)\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-108.1672\n42.123\n-2.568\n0.037\n-207.772\n-8.563\n\n\nage\n0.3291\n0.069\n4.753\n0.002\n0.165\n0.493\n\n\nheight\n0.9553\n0.244\n3.914\n0.006\n0.378\n1.532\n\n\n\n\n\n\n\n\n2.1.6 Ex 2.10 - 2.11 Mean prediction, confidence interval and prediction interval\nVariance-covariance matrix of coefficients:\n\nmodel_fit.cov_params()\n\n\n\n\n\n\n\n\n\nIntercept\nage\nheight\n\n\n\n\nIntercept\n1774.328062\n-0.671107\n-10.264885\n\n\nage\n-0.671107\n0.004795\n0.003035\n\n\nheight\n-10.264885\n0.003035\n0.059567\n\n\n\n\n\n\n\n\nCreate new data to predict:\n\nnewdata = pd.DataFrame({'age': [40, ], 'height': [170, ]})\n\nPredict mean response by calling predict() method.\n\nmodel_fit.predict(newdata)\n\n0    67.397178\ndtype: float64\n\n\nLet’s also get 95% confidence interval of mean response and 95% prediction interval. To do so, first let’s produce a prediction object by calling get_prediction() method.\n\nprediction_results = model_fit.get_prediction(newdata)\n\nsummary_frame() method produces data frame that include mean prediction, confidence interval, and prediction interval. To compute 95% confidence/prediction intervals, pass alpha=0.95 as an argument.\n\nprediction_results.summary_frame(alpha=0.95)\n\n\n\n\n\n\n\n\n\nmean\nmean_se\nmean_ci_lower\nmean_ci_upper\nobs_ci_lower\nobs_ci_upper\n\n\n\n\n0\n67.397178\n1.006575\n67.331762\n67.462594\n67.21277\n67.581587",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "ch02_regression.html#example-2.14-2.16",
    "href": "ch02_regression.html#example-2.14-2.16",
    "title": "2  Regression",
    "section": "2.2 Example 2.14, 2.16",
    "text": "2.2 Example 2.14, 2.16\n\n2.2.1 Load data\n\ndat1 = pd.read_csv(\"data/ch2_coil.csv\")\ndat1\n\n\n\n\n\n\n\n\n\ntemp\nthick\ny\n\n\n\n\n0\n540\n2\n52.5\n\n\n1\n660\n2\n50.2\n\n\n2\n610\n2\n51.3\n\n\n3\n710\n2\n49.1\n\n\n4\n570\n6\n50.8\n\n\n5\n700\n6\n48.7\n\n\n6\n560\n6\n51.2\n\n\n7\n600\n6\n50.8\n\n\n8\n680\n6\n49.3\n\n\n9\n530\n6\n51.5\n\n\n\n\n\n\n\n\n\n\n2.2.2 Ex 2.14: Estimate a model with an indicator variable\nUsing C() inside a formula means that the variable will be considered as a categorical variable. In such case, dummy variables are automatically created during the process of model estimation. Inside C(), you can pass the second argument Treatment(reference=6) to specify 6 as a base category, so the dummy variable value is 0 when original variable value is 6.\n\nmodel_fit = smf.ols(\"y ~ temp + C(thick, Treatment(reference=6))\", data=dat1).fit()\n\nLet’s check the estimated regression coefficients.\n\nmodel_fit.summary().tables[1]\n\n/opt/hostedtoolcache/Python/3.11.9/x64/lib/python3.11/site-packages/scipy/stats/_axis_nan_policy.py:531: UserWarning: kurtosistest only valid for n&gt;=20 ... continuing anyway, n=10\n  res = hypotest_fun_out(*samples, **kwds)\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n61.1080\n0.703\n86.864\n0.000\n59.444\n62.771\n\n\nC(thick, Treatment(reference=6))[T.2]\n0.8042\n0.150\n5.376\n0.001\n0.450\n1.158\n\n\ntemp\n-0.0177\n0.001\n-15.380\n0.000\n-0.020\n-0.015\n\n\n\n\n\n\n\n\n2.2.3 Ex 2.16: Interaction term\nUse * to estimate not only main effects but only interactions.\n\nmodel_fit = smf.ols(\"y ~ temp*C(thick, Treatment(reference=6))\", data=dat1).fit()\nmodel_fit.summary().tables[1]\n\n/opt/hostedtoolcache/Python/3.11.9/x64/lib/python3.11/site-packages/scipy/stats/_axis_nan_policy.py:531: UserWarning: kurtosistest only valid for n&gt;=20 ... continuing anyway, n=10\n  res = hypotest_fun_out(*samples, **kwds)\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n60.1364\n0.750\n80.187\n0.000\n58.301\n61.971\n\n\nC(thick, Treatment(reference=6))[T.2]\n3.2785\n1.210\n2.708\n0.035\n0.317\n6.240\n\n\ntemp\n-0.0161\n0.001\n-13.074\n0.000\n-0.019\n-0.013\n\n\ntemp:C(thick, Treatment(reference=6))[T.2]\n-0.0040\n0.002\n-2.055\n0.086\n-0.009\n0.001",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "ch03_regularized_regression.html",
    "href": "ch03_regularized_regression.html",
    "title": "3  Regularized regression",
    "section": "",
    "text": "3.1 Examples 3.1 - 3.2",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regularized regression</span>"
    ]
  },
  {
    "objectID": "ch03_regularized_regression.html#examples-3.1---3.2",
    "href": "ch03_regularized_regression.html#examples-3.1---3.2",
    "title": "3  Regularized regression",
    "section": "",
    "text": "3.1.1 Load data\n\ndat1 = pd.read_csv(\"data/ch3_dat1.csv\")\ndat1\n\n\n\n\n\n\n\n\n\nx1\nx2\ny\n\n\n\n\n0\n-6\n-3\n-3\n\n\n1\n-4\n-1\n-2\n\n\n2\n-3\n-2\n-1\n\n\n3\n1\n-4\n0\n\n\n4\n2\n1\n1\n\n\n5\n3\n2\n2\n\n\n6\n7\n7\n3\n\n\n\n\n\n\n\n\n\n\n3.1.2 Standardize inputs\nLet us convert data frame into input data matrix x and output array y. Additional, let N be the number of training data.\n\nx = dat1[['x1', 'x2']].to_numpy()\ny = dat1['y'].to_numpy()\nN = len(y)\n\n\nstd_x = (x - x.mean(axis=0)) / x.std(axis=0, ddof=1)\n\n\n\n3.1.3 Ex 3.1: Lasso\n\n3.1.3.1 statsmodels\nFirst, let us estimate a model by using {statsmodels}. After defining an OLS model, use fit_regularized() method with L1_wt=1 to give penalty on the L1-norm of cofficients.\n\ndef lasso(X, y, alpha):\n    model_fit = (sm\n                 .OLS(y, X)\n                 .fit_regularized(alpha=alpha, L1_wt=1))\n\n    return model_fit\n\nLet us fit models by varying penalization size.\n\nlambdas = np.arange(4) / (2 * N)\nlasso_fit = [lasso(std_x, y, l) for l in lambdas]\n[fit.params for fit in lasso_fit]\n\n[array([2.02007827, 0.1339426 ]),\n array([1.97361834, 0.08748266]),\n array([1.9271584 , 0.04102273]),\n array([1.87638317, 0.        ])]\n\n\n\n\n3.1.3.2 scikit-learn\n{scikit-learn} provides ElasticNet() to fit regularized regression model. Pass l1_ratio=1 to fit a Lasso model.\n\nlambdas = np.arange(4) / (2 * N)\nlasso_model = [ElasticNet(alpha=l, l1_ratio=1) for l in lambdas]\nlasso_fit = [model.fit(std_x, y) for model in lasso_model]\n[fit.coef_ for fit in lasso_fit]\n\n/opt/hostedtoolcache/Python/3.11.9/x64/lib/python3.11/site-packages/sklearn/base.py:1473: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n  return fit_method(estimator, *args, **kwargs)\n/opt/hostedtoolcache/Python/3.11.9/x64/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:697: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n  model = cd_fast.enet_coordinate_descent(\n/opt/hostedtoolcache/Python/3.11.9/x64/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.156e-01, tolerance: 2.800e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n  model = cd_fast.enet_coordinate_descent(\n\n\n[array([2.02007827, 0.1339426 ]),\n array([1.97388922, 0.08726767]),\n array([1.92747855, 0.04076864]),\n array([1.87638317, 0.        ])]\n\n\n\n\n\n3.1.4 Ex 3.2: Ridge\nDo similarly as Lasso, but use different argument L1_wt=0 and l1_ratio=0.\n\n3.1.4.1 statsmodels\n\ndef ridge(X, y, alpha):\n    model_fit = (sm\n                 .OLS(y, X)\n                 .fit_regularized(alpha=alpha, L1_wt=0))\n\n    return model_fit\n\n\nlambdas = np.arange(4) / N\nridge_fit = [ridge(std_x, y, l) for l in lambdas]\n[fit.params for fit in ridge_fit]\n\n[array([2.02007827, 0.1339426 ]),\n array([1.5071298 , 0.46375656]),\n array([1.2688007 , 0.54765121]),\n array([1.11772476, 0.56673637])]\n\n\n\n\n3.1.4.2 scikit-learn\n\nlambdas = np.arange(4) / N\nlasso_model = [ElasticNet(alpha=l, l1_ratio=0) for l in lambdas]\nlasso_fit = [model.fit(std_x, y) for model in lasso_model]\n[fit.coef_ for fit in lasso_fit]\n\n/opt/hostedtoolcache/Python/3.11.9/x64/lib/python3.11/site-packages/sklearn/base.py:1473: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n  return fit_method(estimator, *args, **kwargs)\n/opt/hostedtoolcache/Python/3.11.9/x64/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:697: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n  model = cd_fast.enet_coordinate_descent(\n/opt/hostedtoolcache/Python/3.11.9/x64/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.156e-01, tolerance: 2.800e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n  model = cd_fast.enet_coordinate_descent(\n/opt/hostedtoolcache/Python/3.11.9/x64/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.969e+00, tolerance: 2.800e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n  model = cd_fast.enet_coordinate_descent(\n/opt/hostedtoolcache/Python/3.11.9/x64/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.052e+00, tolerance: 2.800e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n  model = cd_fast.enet_coordinate_descent(\n/opt/hostedtoolcache/Python/3.11.9/x64/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.916e+00, tolerance: 2.800e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n  model = cd_fast.enet_coordinate_descent(\n\n\n[array([2.02007827, 0.1339426 ]),\n array([1.5071298 , 0.46375656]),\n array([1.2688007 , 0.54765121]),\n array([1.11772476, 0.56673637])]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regularized regression</span>"
    ]
  },
  {
    "objectID": "ch04_dimension_reduction.html",
    "href": "ch04_dimension_reduction.html",
    "title": "4  Dimension reduction",
    "section": "",
    "text": "4.1 Example 4.10",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension reduction</span>"
    ]
  },
  {
    "objectID": "ch04_dimension_reduction.html#example-4.10",
    "href": "ch04_dimension_reduction.html#example-4.10",
    "title": "4  Dimension reduction",
    "section": "",
    "text": "4.1.1 Load data\n\ndat2 = pd.read_csv(\"data/ch4_dat2.csv\", encoding=\"euc-kr\")\ndat2\n\n\n\n\n\n\n\n\n\nID\nX1\nX2\nX3\nX4\nX5\n\n\n\n\n0\nSK증권\n2.43\n11.10\n18.46\n441.67\n0.90\n\n\n1\n교보증권\n3.09\n9.95\n29.46\n239.43\n0.90\n\n\n2\n대신증권\n2.22\n6.86\n28.62\n249.36\n0.69\n\n\n3\n대우증권\n5.76\n23.19\n23.47\n326.09\n1.43\n\n\n4\n동부증권\n1.60\n5.64\n25.64\n289.98\n1.42\n\n\n5\n메리츠증권\n3.53\n10.64\n32.25\n210.10\n1.17\n\n\n6\n미래에셋증권\n4.26\n15.56\n24.40\n309.78\n0.81\n\n\n7\n부국증권\n3.86\n5.50\n70.74\n41.36\n0.81\n\n\n8\n브릿지증권\n4.09\n6.44\n64.38\n55.32\n0.32\n\n\n9\n삼성증권\n2.73\n10.68\n24.41\n309.59\n0.64\n\n\n10\n서울증권\n2.03\n4.50\n42.53\n135.12\n0.59\n\n\n11\n신영증권\n1.96\n8.92\n18.48\n441.19\n1.07\n\n\n12\n신흥증권\n3.25\n7.96\n40.42\n147.41\n1.19\n\n\n13\n우리투자증권\n2.01\n10.28\n17.46\n472.78\n1.25\n\n\n14\n유화증권\n2.28\n3.65\n63.71\n56.96\n0.12\n\n\n15\n한양증권\n4.51\n7.50\n63.52\n57.44\n0.80\n\n\n16\n한화증권\n3.29\n12.37\n24.47\n308.63\n0.57\n\n\n17\n현대증권\n1.73\n7.57\n19.59\n410.45\n1.19\n\n\n\n\n\n\n\n\n\n\n4.1.2 Principal component analysis with statsmodels\nTrain a PCA model.\n\npca_model = sm.PCA(dat2.drop(\"ID\", axis=1))\n\nPrint a loading matrix.\n\npca_model.loadings\n\n\n\n\n\n\n\n\n\ncomp_0\ncomp_1\ncomp_2\ncomp_3\ncomp_4\n\n\n\n\nX1\n0.076084\n0.779670\n0.000892\n0.140755\n0.605403\n\n\nX2\n-0.394630\n0.565412\n-0.295322\n-0.117644\n-0.650785\n\n\nX3\n0.569702\n0.162282\n0.241222\n0.637722\n-0.429217\n\n\nX4\n-0.559828\n-0.196543\n-0.256597\n0.748094\n0.149922\n\n\nX5\n-0.447785\n0.086368\n0.888118\n0.003668\n-0.057115\n\n\n\n\n\n\n\n\nDraw a scree plot.\n\nfig = pca_model.plot_scree(log_scale=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe x-axis represents number of principal component - 1. 0 in the x-axis represents the 1st principal component.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe y-axis values are different from the book example, because statsmodels returns eigenvalues multiplied by the number of observations.\n\nprint(\"Eigenvalue from statsmodels:\")\nprint(pca_model.eigenvals)\n\nprint(\"Eigenvalue in the book example:\")\nprint(pca_model.eigenvals / len(dat2))\n\nEigenvalue from statsmodels:\n0    49.706320\n1    28.901757\n2     9.910135\n3     1.153137\n4     0.328651\nName: eigenvals, dtype: float64\nEigenvalue in the book example:\n0    2.761462\n1    1.605653\n2    0.550563\n3    0.064063\n4    0.018258\nName: eigenvals, dtype: float64\n\n\n\n\nContribution\n\npca_model.rsquare\n\nncomp\n0    0.000000\n1    0.552292\n2    0.873423\n3    0.983536\n4    0.996348\n5    1.000000\nName: rsquare, dtype: float64",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension reduction</span>"
    ]
  }
]